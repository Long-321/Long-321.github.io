<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>【命令】相关集群开启命令</title>
    <link href="/2025/06/07/%E3%80%90%E5%91%BD%E4%BB%A4%E3%80%91%E7%9B%B8%E5%85%B3%E9%9B%86%E7%BE%A4%E5%BC%80%E5%90%AF%E5%91%BD%E4%BB%A4/"/>
    <url>/2025/06/07/%E3%80%90%E5%91%BD%E4%BB%A4%E3%80%91%E7%9B%B8%E5%85%B3%E9%9B%86%E7%BE%A4%E5%BC%80%E5%90%AF%E5%91%BD%E4%BB%A4/</url>
    
    <content type="html"><![CDATA[<h2 id="Hadoop"><a href="#Hadoop" class="headerlink" title="Hadoop"></a>Hadoop</h2><h3 id="Hdfs"><a href="#Hdfs" class="headerlink" title="Hdfs"></a>Hdfs</h3><blockquote><p>NameNode：元数据<br>DataNode：数据块</p></blockquote><figure class="highlight livecodeserver"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs livecodeserver"><span class="hljs-built_in">start</span>-dfs.sh <span class="hljs-comment"> // 启动hdfs</span><br>hadoop-daemon.sh <span class="hljs-built_in">start</span> namenode<span class="hljs-comment"> // hdfs和yarn通用</span><br></code></pre></td></tr></table></figure><h3 id="Yarn"><a href="#Yarn" class="headerlink" title="Yarn"></a>Yarn</h3><blockquote><p>ResourceManager：集群资源<br>NodeManager：节点资源</p></blockquote><figure class="highlight livecodeserver"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs livecodeserver"><span class="hljs-built_in">start</span>-yarn.sh <span class="hljs-comment"> // 启动yarn</span><br>hadoop-daemon.sh <span class="hljs-built_in">start</span> namenode<span class="hljs-comment"> // hdfs和yarn通用</span><br></code></pre></td></tr></table></figure><h2 id="Zookeeper"><a href="#Zookeeper" class="headerlink" title="Zookeeper"></a>Zookeeper</h2><blockquote><p>每一台机器都要启动 zkServer.sh start  </p></blockquote><h2 id="Hive"><a href="#Hive" class="headerlink" title="Hive"></a>Hive</h2><blockquote><p>hadoop集群开启后 hive</p></blockquote><h2 id="HBase-16010"><a href="#HBase-16010" class="headerlink" title="HBase 16010"></a>HBase 16010</h2><blockquote><p>HMaster<br>HRegionServer<br>Hdfs集群开启后 start-hbase.sh</p></blockquote><h2 id="Flume"><a href="#Flume" class="headerlink" title="Flume"></a>Flume</h2><blockquote><p>配置文件 net-flume-logger.conf 后<br>flume-ng agent<br>-n a1<br>-c ~&#x2F;flume&#x2F;conf<br>-f ~&#x2F;flume&#x2F;conf&#x2F;net-flume-logger.conf<br>-Dflume.root.logger&#x3D;INFO,console</p></blockquote><h2 id="Kafka"><a href="#Kafka" class="headerlink" title="Kafka"></a>Kafka</h2><blockquote><p>Zookeeper集群开启后<br>kafka-server-start.sh -daemon ~&#x2F;kafka&#x2F;config&#x2F;server.properties</p></blockquote><h2 id="Spark"><a href="#Spark" class="headerlink" title="Spark"></a>Spark</h2><blockquote><p>~&#x2F;spark&#x2F;sbin&#x2F;start-all.sh</p></blockquote><h2 id="Flink"><a href="#Flink" class="headerlink" title="Flink"></a>Flink</h2><blockquote><p>启动Hadoop集群后 start-cluster.sh</p></blockquote><h2 id="Davinci"><a href="#Davinci" class="headerlink" title="Davinci"></a>Davinci</h2><blockquote><p>start-server.sh</p></blockquote>]]></content>
    
    
    <categories>
      
      <category>大数据</category>
      
      <category>Hadoop</category>
      
    </categories>
    
    
    <tags>
      
      <tag>集群</tag>
      
      <tag>命令</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>idea-2025破解</title>
    <link href="/2025/06/07/idea-2025%E7%A0%B4%E8%A7%A3/"/>
    <url>/2025/06/07/idea-2025%E7%A0%B4%E8%A7%A3/</url>
    
    <content type="html"><![CDATA[<h2 id="idea2025破解"><a href="#idea2025破解" class="headerlink" title="idea2025破解"></a>idea2025破解</h2><p><a href="https://www.jetbrains.com/idea/download/?section=windows">https://www.jetbrains.com/idea/download/?section=windows</a></p><blockquote><p>双击 .exe 安装包开始安装 IDEA；<br>自定义 IDEA 的安装路径，这里我选择安装在了 E:&#x2F; 盘下，点击下一步按钮；<br>勾选创建桌面快捷方式，点击下一步按钮；<br>点击安装按钮；<br>等待安装成功…；<br>安装成功后，先不要勾选运行 IDEA, 直接点击完成按钮，完毕弹框，准备开始进入破解步骤；</p></blockquote><h2 id="破解脚本"><a href="#破解脚本" class="headerlink" title="破解脚本"></a>破解脚本</h2><p><a href="https://pan.baidu.com/s/1EDtR94PN9jdTi3MaSyjbuw?pwd=ygm2">https://pan.baidu.com/s/1EDtR94PN9jdTi3MaSyjbuw?pwd=ygm2</a></p><blockquote><p>不要放在中文目录下，否则会报错。<br>点击启动，比如：”E:\software\jetbra-pojie\win2021-2025\scripts\install-current-user.vbs”<br>双击 install-current-user.vbs 脚本 ，表示为当前系统用户执行破解。然后，点击弹框中的确定按钮，需要等待个 10s - 30s 时间（部分用户可能需要等待好几分钟，本人就是，所以，耐心等待一会就行）。</p></blockquote><h2 id="激活码"><a href="#激活码" class="headerlink" title="激活码"></a>激活码</h2><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs awk">ZCB571FZHV-eyJsaWNlbnNlSWQiOiJaQ0I1NzFGWkhWIiwibGljZW5zZWVOYW1lIjoiZnV6emVzIGFsbHkiLCJhc3NpZ25lZU5hbWUiOiIiLCJhc3NpZ25lZUVtYWlsIjoiIiwibGljZW5zZVJlc3RyaWN0aW9uIjoiIiwiY2hlY2tDb25jdXJyZW50VXNlIjpmYWxzZSwicHJvZHVjdHMiOlt7ImNvZGUiOiJQREIiLCJmYWxsYmFja0RhdGUiOiIyMDIzLTA3LTAxIiwicGFpZFVwVG8iOiIyMDIzLTA3LTAxIiwiZXh0ZW5kZWQiOnRydWV9LHsiY29kZSI6IlBTSSIsImZhbGxiYWNrRGF0ZSI6IjIwMjMtMDctMDEiLCJwYWlkVXBUbyI6IjIwMjMtMDctMDEiLCJleHRlbmRlZCI6dHJ1ZX0seyJjb2RlIjoiUFBDIiwiZmFsbGJhY2tEYXRlIjoiMjAyMy0wNy0wMSIsInBhaWRVcFRvIjoiMjAyMy0wNy0wMSIsImV4dGVuZGVkIjp0cnVlfSx7ImNvZGUiOiJQQ1dNUCIsImZhbGxiYWNrRGF0ZSI6IjIwMjMtMDctMDEiLCJwYWlkVXBUbyI6IjIwMjMtMDctMDEiLCJleHRlbmRlZCI6dHJ1ZX0seyJjb2RlIjoiUFBTIiwiZmFsbGJhY2tEYXRlIjoiMjAyMy0wNy0wMSIsInBhaWRVcFRvIjoiMjAyMy0wNy0wMSIsImV4dGVuZGVkIjp0cnVlfSx7ImNvZGUiOiJQUkIiLCJmYWxsYmFja0RhdGUiOiIyMDIzLTA3LTAxIiwicGFpZFVwVG8iOiIyMDIzLTA3LTAxIiwiZXh0ZW5kZWQiOnRydWV9LHsiY29kZSI6IklJIiwiZmFsbGJhY2tEYXRlIjoiMjAyMy0wNy0wMSIsInBhaWRVcFRvIjoiMjAyMy0wNy0wMSIsImV4dGVuZGVkIjpmYWxzZX0seyJjb2RlIjoiUEdPIiwiZmFsbGJhY2tEYXRlIjoiMjAyMy0wNy0wMSIsInBhaWRVcFRvIjoiMjAyMy0wNy0wMSIsImV4dGVuZGVkIjp0cnVlfSx7ImNvZGUiOiJQU1ciLCJmYWxsYmFja0RhdGUiOiIyMDIzLTA3LTAxIiwicGFpZFVwVG8iOiIyMDIzLTA3LTAxIiwiZXh0ZW5kZWQiOnRydWV9LHsiY29kZSI6IlBXUyIsImZhbGxiYWNrRGF0ZSI6IjIwMjMtMDctMDEiLCJwYWlkVXBUbyI6IjIwMjMtMDctMDEiLCJleHRlbmRlZCI6dHJ1ZX1dLCJtZXRhZGF0YSI6IjAxMjAyMjA3MDFQU0FOMDAwMDA1IiwiaGFzaCI6IlRSSUFMOi01OTQ5ODgxMjIiLCJncmFjZVBlcmlvZERheXMiOjcsImF1dG9Qcm9sb25nYXRlZCI6ZmFsc2UsImlzQXV0b1Byb2xvbmdhdGVkIjpmYWxzZX0=-JNpWl3tvfBw9nYALTrBlJzoryrKHhqmiBxP5IljC6Hlgmb6YlOH8vPngzoyLYa+cGDMVj6fipEpm+BEqIA7oAoBYSu1ZPdzkHAa94apJg+CUQwuw+EJaATdKTANuKYTBsay6WsnrUh8vbIaJpGz19z+uOAc4xRP+gtuyjiwkNECZ6Y9qD+Dx3Gm5xXI3UvKqjPYIhXk23n1pjlxFIUmhD7BumdxF8JHmJJhd<span class="hljs-regexp">/K5FaXQU/</span>K9pMp70GfmSS2KJgxm6SXfslWs<span class="hljs-regexp">/bF5GTY3i1GA6ez05ZyJwsmJMZ1v6W7GWrWNHDLK7i7aXhOLdK9u+pCz+2FpKmadRznpSmixDzj37ig==-MIIETDCCAjSgAwIBAgIBDTANBgkqhkiG9w0BAQsFADAYMRYwFAYDVQQDDA1KZXRQcm9maWxlIENBMB4XDTIwMTAxOTA5MDU1M1oXDTIyMTAyMTA5MDU1M1owHzEdMBsGA1UEAwwUcHJvZDJ5LWZyb20tMjAyMDEwMTkwggEiMA0GCSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQCUlaUFc1wf+CfY9wzFWEL2euKQ5nswqb57V8QZG7d7RoR6rwYUIXseTOAFq210oMEe++LCjzKDuqwDfsyhgDNTgZBPAaC4vUU2oy+XR+Fq8nBixWIsH668HeOnRK6RRhsr0rJzRB95aZ3EAPzBuQ2qPaNGm17pAX0Rd6MPRgjp75IWwI9eA6aMEdPQEVN7uyOtM5zSsjoj79Lbu1fjShOnQZuJcsV8tqnayeFkNzv2LTOlofU/</span>Tbx502Ro073gGjoeRzNvrynAP03pL486P3KCAyiNPhDs2z8<span class="hljs-regexp">/COMrxRlZW5mfzo0xsK0dQGNH3UoG/</span><span class="hljs-number">9</span>RVwHG4eS8LFpMTR9oetHZBAgMBAAGjgZkwgZYwCQYDVR0TBAIwADAdBgNVHQ4EFgQUJNoRIpb1hUHAk0foMSNM9MCEAv8wSAYDVR0jBEEwP4AUo562SGdCEjZBvW3gubSgUouX8bOhHKQaMBgxFjAUBgNVBAMMDUpldFByb2ZpbGUgQ0GCCQDSbLGDsoN54TATBgNVHSUEDDAKBggrBgEFBQcDATALBgNVHQ8EBAMCBaAwDQYJKoZIhvcNAQELBQADggIBABqRoNGxAQct9dQUFK8xqhiZaYPd30TlmCmSAaGJ0eBpvkVeqA2jGYhAQRqFiAlFC63JKvWvRZO1iRuWCEfUMkdqQ9VQPXziE<span class="hljs-regexp">/BlsOIgrL6RlJfuFcEZ8TK3syIfIGQZNCxYhLLUuet2HE6LJYPQ5c0jH4kDooRpcVZ4rBxNwddpctUO2te9UU5/</span>FjhioZQsPvd92qOTsV+<span class="hljs-number">8</span>Cyl2fvNhNKD1Uu9ff5AkVIQn4JU23ozdB<span class="hljs-regexp">/R5oUlebwaTE6WZNBs+TA/</span>qPj+<span class="hljs-number">5</span><span class="hljs-regexp">/we9NH71WRB0hqUoLI2AKKyiPw++FtN4Su1vsdDlrAzDj9ILjpjJKA1ImuVcG329/</span>WTYIKysZ1CWK3zATg9BeCUPAV1pQy8ToXOq+RSYen6winZ2OO93eyHv2Iw5kbn1dqfBw1BuTE29V2FJKicJSu8iEOpfoafwJISXmz1wnnWL3V<span class="hljs-regexp">/0NxTulfWsXugOoLfv0ZIBP1xH9kmf22jjQ2JiHhQZP7ZDsreRrOeIQ/</span>c4yR8IQvMLfC0WKQqrHu5ZzXTH4NO3CwGWSlTY74kE91zXB5mwWAx1jig+UXYc2w4RkVhy0<span class="hljs-regexp">//</span>lOmVya<span class="hljs-regexp">/PEepuuTTI4+UJwC7qbVlh5zfhj8oTNUXgN0AOc+Q0/</span>WFPl1aw5VV<span class="hljs-regexp">/VrO8FCoB15lFVlpKaQ1Yh+DVU8ke+rt9Th0BCHXe0uZOEmH0nOnH/</span><span class="hljs-number">0</span>onD<br></code></pre></td></tr></table></figure><blockquote><p>如果第一次安装idea提示地区别选中国地区，会导致激活失败。</p></blockquote>]]></content>
    
    
    <categories>
      
      <category>破解</category>
      
    </categories>
    
    
    <tags>
      
      <tag>idea</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>【部署】ZooKeeper 分布式协调框架</title>
    <link href="/2025/06/05/%E3%80%90%E9%83%A8%E7%BD%B2%E3%80%91ZooKeeper%20%E5%88%86%E5%B8%83%E5%BC%8F%E5%8D%8F%E8%B0%83%E6%A1%86%E6%9E%B6/"/>
    <url>/2025/06/05/%E3%80%90%E9%83%A8%E7%BD%B2%E3%80%91ZooKeeper%20%E5%88%86%E5%B8%83%E5%BC%8F%E5%8D%8F%E8%B0%83%E6%A1%86%E6%9E%B6/</url>
    
    <content type="html"><![CDATA[<p>Zookeeper分布式集群搭建</p><p>（一）克隆前准备工作</p><p>一、时钟同步</p><p>步骤：</p><p>1、输入date命令可以查看当前系统时间，可以看到此时系统时间为PDT（部分机器或许为EST），并非中国标准时间。我们在中国地区，因此要把系统时间改为CST（中国标准时间）</p><p>2、修改时间类型：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs java">sudo cp /usr/share/zoneinfo/Asia/Shanghai /etc/localtime<br></code></pre></td></tr></table></figure><p>修改完后再次使用date命令查看是否更改成功。</p><p>二、Zookeeper的安装和部署</p><p>步骤：</p><p>1、使用XFTP把安装包zookeeper-3.4.6.tar.gz传输到虚拟机主目录（&#x2F;home&#x2F;hadoop）下，然后解压：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs java">cd ~  <br>tar -zxvf zookeeper-<span class="hljs-number">3.4</span><span class="hljs-number">.6</span>.tar.gz<br></code></pre></td></tr></table></figure><p>2、配置环境变量，在.bashrc文件中添加内容。先执行以下命令打开.bashrc文件：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs java">vim ~/.bashrc<br></code></pre></td></tr></table></figure><p>在文件末尾加上以下语句：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs java">export ZOOKEEPER_HOME=/home/hadoop/zookeeper-<span class="hljs-number">3.4</span><span class="hljs-number">.6</span>  <br>export PATH=$ZOOKEEPER_HOME/bin:$PATH<br></code></pre></td></tr></table></figure><p>添加完成后保存，然后控制台执行以下命令刷新即可：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs java">source ~/.bashrc<br></code></pre></td></tr></table></figure><p>3、修改zoo.cfg配置文件：</p><p>该文件需要新建，并保存在zookeeper安装路径的conf子目录中：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs java">cd ~/zookeeper-<span class="hljs-number">3.4</span><span class="hljs-number">.6</span>/conf  <br>vim zoo.cfg<br></code></pre></td></tr></table></figure><p>4、大概配置内容如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs java">initLimit=<span class="hljs-number">10</span>  <br>syncLimit=<span class="hljs-number">5</span>  <br>#此处为数据保存目录，需自行创建  <br>dataDir=/home/hadoop/zkdata  <br>#此处为日志保存目录，需自行创建  <br>dataLogDir=/home/hadoop/zklog  <br>clientPort=<span class="hljs-number">2181</span>  <br>server<span class="hljs-number">.1</span>=master:<span class="hljs-number">2888</span>:<span class="hljs-number">3888</span>  <br>server<span class="hljs-number">.2</span>=slave1:<span class="hljs-number">2888</span>:<span class="hljs-number">3888</span>  <br>server<span class="hljs-number">.3</span>=slave2:<span class="hljs-number">2888</span>:<span class="hljs-number">3888</span><br></code></pre></td></tr></table></figure><p>[注意：此处标红的两组数字，前面的1、2、3是每个节点服务的编号，后面的主机名是对应节点服务器的IP地址，这里的配置需要和后面克隆后的配置信息对应上。]{.underline}</p><p>[节点服务编号的值是一个整型数字，且不能重复。]{.underline}</p><p>5、创建数据和日志目录：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs java">mkdir /home/hadoop/zkdata  <br>mkdir /home/hadoop/zklog<br></code></pre></td></tr></table></figure><p>此时，可以开始克隆虚拟机了。</p><p>（二）克隆Linux虚拟机</p><p>前言：</p><p>克隆开始之前，可以保存一下虚拟机的快照，或者完整地备份一下自己的虚拟机，这样就不必担忧后续集群使用过程中出现严重的问题需要重来。</p><p>一、克隆出两台虚拟机</p><p>步骤：</p><p>1、使用stop-all.sh命令关闭hadoop集群后，关闭Linux虚拟机；</p><p>2、右键选中master，管理-克隆；</p><p>3、在弹出的克隆虚拟机向导中，选择克隆自虚拟机中的当前状态；</p><p>4、创建完整克隆；</p><p>5、虚拟机名称建议为slave1、slave2；位置选择F盘你自己的目录；</p><p>6、点击完成开始克隆；</p><p>二、把三台虚拟机都启动起来</p><p>步骤：略</p><p>（三）克隆虚拟机后配置</p><p>一、为两台新的虚拟机修改IP地址</p><p>由于是克隆的虚拟机，因此他们的配置都是一样的，所以开机以后第一步，先把两台slave机器的ip地址修改一下，否则IP地址会发生冲突。</p><p>只需修改最后的部分即可，即192.168.203.XXX中标红的一节；设置方式参考前面教程。</p><p>二、为两台新的虚拟机修改主机名</p><p>步骤：</p><p>1、修改slave1主机的&#x2F;etc&#x2F;hostname文件，将其主机名修改为slave1</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs java">sudo vim /etc/hostname<br></code></pre></td></tr></table></figure><p>2、重复上述步骤，也把slave2的主机名修改为slave2</p><p>三、为三台机器配置hosts文件</p><p>步骤：</p><p>1、先在master主机修改&#x2F;etc&#x2F;hosts文件，建立主机名与ip地址的映射：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs java">sudo vim /etc/hosts<br></code></pre></td></tr></table></figure><p>在末尾添加两行：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs java"><span class="hljs-number">192.168</span><span class="hljs-number">.203</span>.XXX slave1  <br><span class="hljs-number">192.168</span><span class="hljs-number">.203</span>.XXX slave2<br></code></pre></td></tr></table></figure><p>其中的ip地址要对应上一步你自己为每一台主机设置的ip地址。</p><p>2、为另外两台slave1、slave2主机也重复上面步骤。</p><p>四、为master主机配置NTP服务器，用于时钟同步</p><p>步骤：</p><p>1、配置NTP服务器：</p><p>我们选择master节点来配置NTP服务器，后续其他节点定时同步master节点的时间。CentOS系统一般已经安装有NTP服务，因此直接修改配置文件ntp.conf即可，该文件位于&#x2F;etc文件夹内：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs java">sudo vim /etc/ntp.conf<br></code></pre></td></tr></table></figure><p>打开后如下：</p><p><img src="https://it.smain.cn/pics/pic_6a2bc9dc.png" alt="pic_6a2bc9dc.png">{width&#x3D;“4.813379265091863in” height&#x3D;“3.261459973753281in”}</p><p>2、修改一：</p><p>找到<code>#restrict 192.168.1.0 mask 255.255.255.0 nomodify notrap</code>这一行，然后在其下方添加：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs java">restrict <span class="hljs-number">192.168</span><span class="hljs-number">.240</span><span class="hljs-number">.129</span> mask <span class="hljs-number">255.255</span><span class="hljs-number">.255</span><span class="hljs-number">.0</span> nomodify notrap<br></code></pre></td></tr></table></figure><p>其中标红的ip地址需改成你master主机的ip地址。</p><p>3、修改二：</p><p>注释掉下面的4行server域名配置：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs java">#server <span class="hljs-number">0.</span>centos.pool.ntp.org iburst  <br>#server <span class="hljs-number">1.</span>centos.pool.ntp.org iburst  <br>#server <span class="hljs-number">2.</span>centos.pool.ntp.org iburst  <br>#server <span class="hljs-number">3.</span>centos.pool.ntp.org iburst<br></code></pre></td></tr></table></figure><p>4、修改三：</p><p>注释完以后在下方添加两行：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs java">server <span class="hljs-number">127.127</span><span class="hljs-number">.1</span><span class="hljs-number">.0</span>  <br>fudge <span class="hljs-number">127.127</span><span class="hljs-number">.1</span><span class="hljs-number">.0</span> stratum <span class="hljs-number">10</span><br></code></pre></td></tr></table></figure><p>5、修改完后大体如图所示：</p><p><img src="https://it.smain.cn/pics/pic_836ce328.png" alt="pic_836ce328.png">{width&#x3D;“4.6292465004374455in” height&#x3D;“1.8431157042869641in”}</p><p>6、在master执行以下命令启动服务：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs java">systemctl start ntpd<br></code></pre></td></tr></table></figure><p>7、执行以下命令可以让master每次启动机器时，都自动开启NTP服务：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs java">chkconfig ntpd on<br></code></pre></td></tr></table></figure><p>8、在其他节点配置定时同步时间：</p><p>使用Linux的crontab命令配置定时任务，定时执行ntpdate时间同步命令即可。</p><p>在slave1、slave2都使用<code>crontab -e</code>命令打开vim编辑器编辑定时命令脚本，在其中添加此行命令：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs java"><span class="hljs-number">0</span>-<span class="hljs-number">59</span>/<span class="hljs-number">10</span> * * * * sudo /usr/sbin/ntpdate master<br></code></pre></td></tr></table></figure><p>其中标红的需改成你上述配置了ntp服务器的主机名。</p><p>设置完成后，后续slave1、slave2将定期每10分钟自动与master主机同步时间。</p><p>五、创建Zookeeper各节点服务编号</p><p>在各个机器的<code>/home/hadoop/zkdata</code>目录，创建一个myid文件，然后分别输入服务编号即可，下面以master为例设置，执行以下命令：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs java">cd /home/hadoop/zkdata  <br>touch myid  <br>echo <span class="hljs-number">1</span> &gt; myid<br></code></pre></td></tr></table></figure><p>其中编号1即为master机器的编号，此处必须按照前面Zookeeper安装与部署步骤4中对应的主机与编号进行设置。</p><p>自行重复上述步骤，把三台机器都配置完毕。</p><p>六、启动Zookeeper集群</p><p>在每一台机器上，分别执行以下命令启动Zookeeper服务：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs java">zkServer.sh start<br></code></pre></td></tr></table></figure><p>一段时间后使用以下命令查看状态：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs java">zkServer.sh status<br></code></pre></td></tr></table></figure><p>如果Zookeeper集群中，出现一个节点为Leader，另外两个为Follower，则说明Zookeeper集群安装部署成功。</p>]]></content>
    
    
    <categories>
      
      <category>大数据</category>
      
      <category>Hadoop</category>
      
    </categories>
    
    
    <tags>
      
      <tag>ZooKeeper</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>【部署】Sqoop 数据迁移工具</title>
    <link href="/2025/06/05/%E3%80%90%E9%83%A8%E7%BD%B2%E3%80%91Sqoop%20%E6%95%B0%E6%8D%AE%E8%BF%81%E7%A7%BB%E5%B7%A5%E5%85%B7/"/>
    <url>/2025/06/05/%E3%80%90%E9%83%A8%E7%BD%B2%E3%80%91Sqoop%20%E6%95%B0%E6%8D%AE%E8%BF%81%E7%A7%BB%E5%B7%A5%E5%85%B7/</url>
    
    <content type="html"><![CDATA[<h2 id="一、Sqoop安装与配置"><a href="#一、Sqoop安装与配置" class="headerlink" title="一、Sqoop安装与配置"></a>一、Sqoop安装与配置</h2><p>步骤：</p><p>1、使用XFTP将Sqoop安装包sqoop-1.4.6.bin__hadoop-2.0.4-alpha.tar.gz发送到master机器的主目录。</p><p>2、解压安装包：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs java">tar -zxvf ~/sqoop-<span class="hljs-number">1.4</span><span class="hljs-number">.6</span>.bin__hadoop-<span class="hljs-number">2.0</span><span class="hljs-number">.4</span>-alpha.tar.gz<br></code></pre></td></tr></table></figure><p>3、修改文件夹的名字，将其改为sqoop，或者创建软连接也可：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs java">mv ~/sqoop-<span class="hljs-number">1.4</span><span class="hljs-number">.6</span>.bin__hadoop-<span class="hljs-number">2.0</span><span class="hljs-number">.4</span>-alpha ~/sqoop<br></code></pre></td></tr></table></figure><p>4、Sqoop的相关配置文件位于安装路径下的conf文件夹中，进入到该目录下，修改相关配置文件即可。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs java">cd ~/sqoop/conf<br></code></pre></td></tr></table></figure><p>5、修改sqoop-env.sh配置文件，该文件一开始并不存在：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs java">vim sqoop-env.sh<br></code></pre></td></tr></table></figure><p>在对应的地方补充上Hadoop、Hive、HBase、ZooKeeper等的安装路径即可，内容如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs java">export HADOOP_COMMON_HOME=/home/hadoop/hadoop-<span class="hljs-number">2.9</span><span class="hljs-number">.2</span><br>export HADOOP_MAPRED_HOME=/home/hadoop/hadoop-<span class="hljs-number">2.9</span><span class="hljs-number">.2</span><br>export HBASE_HOME=/home/hadoop/hbase<br>export HIVE_HOME=/home/hadoop/hive<br>export ZOOCFGDIR=/home/hadoop/zookeeper-<span class="hljs-number">3.4</span><span class="hljs-number">.6</span><br></code></pre></td></tr></table></figure><p>需要根据你的实际情况修改，配置文件修改完成后保存。</p><p>6、配置环境变量，后续只需要在master上启动HBase，因此只在master上配置即可：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs java">vim ~/.bashrc<br></code></pre></td></tr></table></figure><p>在文件末尾添加以下内容：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs java">export SQOOP_HOME=/home/hadoop/sqoop<br>export PATH=$SQOOP_HOME/bin:$PATH<br></code></pre></td></tr></table></figure><p>保存文件，然后刷新环境变量或重新启动命令行终端：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs java">source ~/.bashrc<br></code></pre></td></tr></table></figure><p>7、添加MySQL驱动包，将mysql-connector-java-5.1.38.jar驱动包上传到master主机上，并把它放到Sqoop安装目录下的lib文件夹内即可：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs java">mv ~/mysql-connector-java-<span class="hljs-number">5.1</span><span class="hljs-number">.38</span>.jar ~/sqoop/lib/<br></code></pre></td></tr></table></figure><h2 id="二、测试运行"><a href="#二、测试运行" class="headerlink" title="二、测试运行"></a>二、测试运行</h2><p>前提：确保Zookeeper、HDFS、YARN三个重要组件都正确启动。</p><p>步骤：</p><p>1、使用help命令查看Sqoop的用法，验证安装是否成功：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs java">sqoop help<br></code></pre></td></tr></table></figure><p>2、测试MySQL数据库能否成功连接：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs java">sqoop list-databases --connect jdbc:mysql:<span class="hljs-comment">//localhost --username root -password 123456</span><br></code></pre></td></tr></table></figure><p>如果能打印出本地MySQL数据库里面的数据库名称列表，即为连接本地数据库成功。</p><p><img src="https://it.smain.cn/pics/pic_2c0d4ff8.png" alt="pic_2c0d4ff8.png"></p><p>3、（可选）授予本地MySQL数据库root账户远程访问权限：</p><p>先登录到MySQL中：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs java">mysql -u root -p<br></code></pre></td></tr></table></figure><p>然后执行以下语句：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs java">mysql&gt; grant all on *.* to <span class="hljs-string">&#x27;root&#x27;</span>@<span class="hljs-string">&#x27;master&#x27;</span> identified by <span class="hljs-string">&#x27;123456&#x27;</span> with grant option;<br>mysql&gt; grant all on *.* to <span class="hljs-string">&#x27;root&#x27;</span>@<span class="hljs-string">&#x27;%&#x27;</span> identified by <span class="hljs-string">&#x27;123456&#x27;</span> with grant option;<br>mysql&gt; flush privileges;<br></code></pre></td></tr></table></figure><p>执行完上述语句，就可以把上面连接数据库的localhost，改成主机名或者ip地址，也可以使用root用户远程连接本地的这个MySQL数据库了。</p>]]></content>
    
    
    <categories>
      
      <category>大数据</category>
      
      <category>Hadoop</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Sqoop</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>【部署】Spark 大数据处理框架</title>
    <link href="/2025/06/05/%E3%80%90%E9%83%A8%E7%BD%B2%E3%80%91Spark%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E6%A1%86%E6%9E%B6/"/>
    <url>/2025/06/05/%E3%80%90%E9%83%A8%E7%BD%B2%E3%80%91Spark%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E6%A1%86%E6%9E%B6/</url>
    
    <content type="html"><![CDATA[<h2 id="一、单机模式"><a href="#一、单机模式" class="headerlink" title="一、单机模式"></a>一、单机模式</h2><p>步骤：</p><p>1、使用XFTP将Spark安装包spark-2.4.8-bin-hadoop2.7.tgz发送到master机器的主目录。</p><p>2、解压安装包：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs java">tar -zxvf ~/spark-<span class="hljs-number">2.4</span><span class="hljs-number">.8</span>-bin-hadoop2<span class="hljs-number">.7</span>.tgz<br></code></pre></td></tr></table></figure><p>3、修改文件夹的名字，将其改为flume，或者创建软连接也可：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs java">mv ~/spark-<span class="hljs-number">2.4</span><span class="hljs-number">.8</span>-bin-hadoop2<span class="hljs-number">.7</span> ~/spark<br></code></pre></td></tr></table></figure><p>4、开箱即用；</p><h2 id="二、Spark-Standalone模式集群"><a href="#二、Spark-Standalone模式集群" class="headerlink" title="二、Spark Standalone模式集群"></a>二、Spark Standalone模式集群</h2><p>步骤：</p><p>1、使用vim命令配置Spark的环境配置文件，原本应是不存在的：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs java">cd ~/spark/conf<br><br>vim spark-env.sh<br></code></pre></td></tr></table></figure><p>配置内容如下（注意此处的所有路径、主机名）：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><code class="hljs java"># jdk安装目录<br><br>export JAVA_HOME=/home/hadoop/jdk1<span class="hljs-number">.8</span><span class="hljs-number">.0_311</span><br><br># Hadoop配置文件目录<br><br>export HADOOP_CONF_DIR=/home/hadoop/hadoop-<span class="hljs-number">2.9</span><span class="hljs-number">.2</span>/etc/hadoop<br><br># Hadoop根目录<br><br>export HADOOP_HOME=/home/hadoop/hadoop-<span class="hljs-number">2.9</span><span class="hljs-number">.2</span><br><br># Web UI端口号<br><br>SPARK_MASTER_WEBUI_PORT=<span class="hljs-number">8888</span><br><br># 配置ZooKeeper<br><br>SPARK_DAEMON_JAVA_OPTS=<span class="hljs-string">&quot;-Dspark.deploy.recoveryMode=ZOOKEEPER -Dspark.deploy.zookeeper.url=master:2181,slave1:2181,slave2:2181 -Dspark.deploy.zookeeper.dir=/myspark&quot;</span><br></code></pre></td></tr></table></figure><p>修改完后保存。</p><p>2、配置slaves：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs java">vim slaves<br></code></pre></td></tr></table></figure><p>添加以下内容：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs java">master<br>slave1<br>slave2<br></code></pre></td></tr></table></figure><p>3、接着把配置好的Spark安装目录使用scp命令发送到其他节点：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs java">scp -r ~/spark hadoop<span class="hljs-meta">@slave1</span>:~<br><br>scp -r ~/spark hadoop<span class="hljs-meta">@slave2</span>:~<br></code></pre></td></tr></table></figure><p>4、启动ZooKeeper，略；</p><p>5、在master节点上，带路径启动Spark集群：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs java">~/spark/sbin/start-all.sh<br></code></pre></td></tr></table></figure><p>测试</p><p>浏览器输入：master:8888</p><h2 id="三、Spark-on-YARN模式"><a href="#三、Spark-on-YARN模式" class="headerlink" title="三、Spark on YARN模式"></a>三、Spark on YARN模式</h2><p>实际上，Spark on YARN模式，即把Spark应用程序跑在YARN集群之上，通过第二节配置好Spark Standalone模式后，已经可以在任意节点上，执行spark-submit脚本把任务提交至YARN实现Spark on YARN。而区别就是使用这种方式提交任务的话，就不需要启动Spark集群了。</p>]]></content>
    
    
    <categories>
      
      <category>大数据</category>
      
      <category>Hadoop</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Spark</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>【部署】Kafka 分布式消息队列系统</title>
    <link href="/2025/06/05/%E3%80%90%E9%83%A8%E7%BD%B2%E3%80%91Kafka%20%E5%88%86%E5%B8%83%E5%BC%8F%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97%E7%B3%BB%E7%BB%9F/"/>
    <url>/2025/06/05/%E3%80%90%E9%83%A8%E7%BD%B2%E3%80%91Kafka%20%E5%88%86%E5%B8%83%E5%BC%8F%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97%E7%B3%BB%E7%BB%9F/</url>
    
    <content type="html"><![CDATA[<h2 id="一、Kafka安装与配置"><a href="#一、Kafka安装与配置" class="headerlink" title="一、Kafka安装与配置"></a>一、Kafka安装与配置</h2><p>步骤：</p><p>1、使用XFTP将Kafka安装包kafka_2.12-2.8.1.tgz发送到master机器的主目录。</p><p>2、解压安装包：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs java">tar -zxvf ~/kafka_2<span class="hljs-number">.12</span>-<span class="hljs-number">2.8</span><span class="hljs-number">.1</span>.tgz<br></code></pre></td></tr></table></figure><p>3、修改文件夹的名字，将其改为kafka，或者创建软连接也可：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs java">mv ~/kafka_2<span class="hljs-number">.12</span>-<span class="hljs-number">2.8</span><span class="hljs-number">.1</span> ~/kafka<br></code></pre></td></tr></table></figure><p>4、修改配置文件，Kafka包含生产者、消费者、ZooKeeper、Kafka集群节点等四个角色，因此需要修改对应的4个配置文件即可。Kafka的配置文件夹在解压文件夹里面的config文件夹内，先执行以下命令进入到对应的目录：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs java">cd ~/kafka/config<br></code></pre></td></tr></table></figure><p>（1）修改zookeeper.properties配置文件：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs java"># 指定ZooKeeper的数据目录<br>dataDir=/home/hadoop/zkdata<br># 指定ZooKeeper的端口（这个应该不用改）<br>clientPort=<span class="hljs-number">2181</span><br></code></pre></td></tr></table></figure><p>（2）修改consumer.properties配置文件：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs java"># 配置Kafka集群地址<br>bootstrap.servers=master:<span class="hljs-number">9092</span>,slave1:<span class="hljs-number">9092</span>,slave2:<span class="hljs-number">9092</span><br></code></pre></td></tr></table></figure><p>（3）修改producer.properties配置文件：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs java"># 配置Kafka集群地址<br>bootstrap.servers=master:<span class="hljs-number">9092</span>,slave1:<span class="hljs-number">9092</span>,slave2:<span class="hljs-number">9092</span><br></code></pre></td></tr></table></figure><p>（4）修改server.properties配置文件（找到对应的参数修改）：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs java"># 指定ZooKeeper集群<br>zookeeper.connect=master:<span class="hljs-number">2181</span>,slave1:<span class="hljs-number">2181</span>,slave2:<span class="hljs-number">2181</span><br></code></pre></td></tr></table></figure><p>上述内容修改完毕后保存。</p><p>5、将上面配置修改好后的Kafka安装文件夹使用scp命令分发给slave1、slave2两个节点：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs java">scp -r ~/kafka hadoop<span class="hljs-meta">@slave1</span>:~<br>scp -r ~/kafka hadoop<span class="hljs-meta">@slave2</span>:~<br></code></pre></td></tr></table></figure><p>6、分发完成以后，在每个节点上修改server编号：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs java">cd ~/kafka/config<br>vim server.properties<br></code></pre></td></tr></table></figure><p>找到broker.id项，master、slave1、slave2三台机器分别标识为1、2、3：</p><p>broker.id&#x3D;1</p><p>broker.id&#x3D;2</p><p>broker.id&#x3D;3</p><p>7、配置环境变量，三台机器都要配置：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs java">vim ~/.bashrc<br></code></pre></td></tr></table></figure><p>在文件末尾添加以下内容：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs java">export KAFKA_HOME=/home/hadoop/kafka<br>export PATH=$KAFKA_HOME/bin:$PATH<br></code></pre></td></tr></table></figure><p>保存文件，然后刷新环境变量或重新启动命令行终端：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs java">source ~/.bashrc<br></code></pre></td></tr></table></figure><h2 id="二、集群启动测试"><a href="#二、集群启动测试" class="headerlink" title="二、集群启动测试"></a>二、集群启动测试</h2><p>Kafka作为一个分布式应用程序，它依赖ZooKeeper提供协调服务，因此要启动Kafka，必须先启动ZooKeeper集群，再启动Kafka集群。</p><p>步骤：</p><p>1、启动好ZooKeeper以后，在三台机器上执行以下命令启动Kafka：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs java">kafka-server-start.sh -daemon ~/kafka/config/server.properties<br></code></pre></td></tr></table></figure><p>用jps命令在三台机器上都能看到Kafka进程的话，说明Kafka集群服务启动成功。</p><h2 id="三、集群功能测试"><a href="#三、集群功能测试" class="headerlink" title="三、集群功能测试"></a>三、集群功能测试</h2><p>集群启动成功后，还需测试其功能是否能正确使用，Kafka自带很多种Shell脚本供用户使用，包括生产消息、消费消息、Topic管理等，下面利用Shell脚本来测试Kafka集群的功能。</p><p>步骤：</p><p>1、创建Topic：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs java">kafka-topics.sh --zookeeper localhost:<span class="hljs-number">2181</span> --create --topic test --replication-factor <span class="hljs-number">3</span> --partitions <span class="hljs-number">3</span><br></code></pre></td></tr></table></figure><p>2、查看Topic列表：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs java">kafka-topics.sh --zookeeper localhost:<span class="hljs-number">2181</span> --list<br></code></pre></td></tr></table></figure><p>能够看到第1步创建的test则创建Topic成功。</p><p>3、启动消费者：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs java">kafka-console-consumer.sh --bootstrap-server localhost:<span class="hljs-number">9092</span> --topic test<br></code></pre></td></tr></table></figure><p>这样就打开了一个终端消费者，消费test Topic中的消息，但目前该Topic中还没有消息，因此这个进程会没有任何反应，这是正常的。</p><p>4、另外打开一个终端，启动生产者向test Topic发送消息：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs java">kafka-console-producer.sh --broker-list localhost:<span class="hljs-number">9092</span> --topic test<br></code></pre></td></tr></table></figure><p>在这个生产者终端上输入消息，然后按回车发送，发送完之后，如果刚才那个消费者终端上，能够查看到消费了对应的消息的话，说明Kafka集群可以正常对消息进行生产和消费了。</p><p>生产者slave1发送消息</p><p>同时,消费者master接受到消息</p>]]></content>
    
    
    <categories>
      
      <category>大数据</category>
      
      <category>Hadoop</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Kafka</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>【部署】Hive 数据仓库</title>
    <link href="/2025/06/05/%E3%80%90%E9%83%A8%E7%BD%B2%E3%80%91Hive%20%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/"/>
    <url>/2025/06/05/%E3%80%90%E9%83%A8%E7%BD%B2%E3%80%91Hive%20%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/</url>
    
    <content type="html"><![CDATA[<h2 id="一、MySQL的安装与配置"><a href="#一、MySQL的安装与配置" class="headerlink" title="一、MySQL的安装与配置"></a>一、MySQL的安装与配置</h2><p>换源：</p><p>最下面附加部分<br>1、在master上直接使用yum命令在线安装MySQL数据库：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs java">sudo yum install mysql-server<br></code></pre></td></tr></table></figure><p>途中会询问是否继续，输入Y并按回车。</p><h3 id="2、启动MySQL服务："><a href="#2、启动MySQL服务：" class="headerlink" title="2、启动MySQL服务："></a>2、启动MySQL服务：</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs java">sudo service mysqld start<br></code></pre></td></tr></table></figure><h3 id="3、设置MySQL的root用户密码："><a href="#3、设置MySQL的root用户密码：" class="headerlink" title="3、设置MySQL的root用户密码："></a>3、设置MySQL的root用户密码：</h3><p>MySQL安装完成后，默认root用户是没有密码的，需要先登录并设置其密码：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs java">mysql -u root -p<br></code></pre></td></tr></table></figure><p>不用输密码直接回车，登录进去以后执行以下命令设置root用户密码：（标红的才是命令，蓝色的123456则是要设置的密码，你可以按需更改）</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs java">mysql&gt;set password <span class="hljs-keyword">for</span> root<span class="hljs-meta">@localhost</span>=password(<span class="hljs-string">&#x27;123456&#x27;</span>);<br></code></pre></td></tr></table></figure><p>尝试quit退出并重新使用root用户登录MySQL，</p><p>如果成功输入密码登录，就说明密码设置成功。  </p><h2 id="二、Hive的安装与配置"><a href="#二、Hive的安装与配置" class="headerlink" title="二、Hive的安装与配置"></a>二、Hive的安装与配置</h2><h3 id="1、先把hive安装包apache-hive-2-3-7-bin-tar-gz发送到master的主目录上，然后在master上解压该安装包："><a href="#1、先把hive安装包apache-hive-2-3-7-bin-tar-gz发送到master的主目录上，然后在master上解压该安装包：" class="headerlink" title="1、先把hive安装包apache-hive-2.3.7-bin.tar.gz发送到master的主目录上，然后在master上解压该安装包："></a>1、先把hive安装包apache-hive-2.3.7-bin.tar.gz发送到master的主目录上，然后在master上解压该安装包：</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs java">cd ~<br>tar -zxvf apache-hive-<span class="hljs-number">2.3</span><span class="hljs-number">.7</span>-bin.tar.gz<br></code></pre></td></tr></table></figure><h3 id="2、由于解压出来的文件夹名字比较长，可以将其改名为hive，或者自行创建软连接也可："><a href="#2、由于解压出来的文件夹名字比较长，可以将其改名为hive，或者自行创建软连接也可：" class="headerlink" title="2、由于解压出来的文件夹名字比较长，可以将其改名为hive，或者自行创建软连接也可："></a>2、由于解压出来的文件夹名字比较长，可以将其改名为hive，或者自行创建软连接也可：</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs java">mv ~/apache-hive-<span class="hljs-number">2.3</span><span class="hljs-number">.7</span>-bin ~/hive<br></code></pre></td></tr></table></figure><h3 id="3、复制hive配置文件："><a href="#3、复制hive配置文件：" class="headerlink" title="3、复制hive配置文件："></a>3、复制hive配置文件：</h3><p>hive的配置文件是hive-site.xml，该文件位于hive安装路径下的conf文件夹中。但一开始该配置文件并不存在，直接使用vim新建一个配置文件即可：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs java">cd ~/hive/conf<br>vim hive-site.xml<br></code></pre></td></tr></table></figure><p>配置文件内容如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><code class="hljs java">&lt;configuration&gt;<br>&lt;property&gt;<br>    &lt;name&gt;hive.exec.local.scratchdir&lt;/name&gt;<br>        &lt;value&gt;/home/hadoop/hive/iotmp&lt;/value&gt;<br>&lt;/property&gt;<br>&lt;property&gt;<br>    &lt;name&gt;hive.downloaded.resources.dir&lt;/name&gt;<br>        &lt;value&gt;/home/hadoop/hive/iotmp&lt;/value&gt;<br>&lt;/property&gt;<br>&lt;property&gt;<br>    &lt;name&gt;hive.querylog.location&lt;/name&gt;<br>        &lt;value&gt;/home/hadoop/hive/iotmp&lt;/value&gt;<br>&lt;/property&gt;<br>#指定连接驱动为MySQL的jdbc<br>&lt;property&gt;<br>    &lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;/name&gt;<br>        &lt;value&gt;com.mysql.jdbc.Driver&lt;/value&gt;<br>&lt;/property&gt;<br>#配置连接MySQL的URL<br>&lt;property&gt;<br>    &lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt;<br>&lt;value&gt;jdbc:mysql:<span class="hljs-comment">//localhost/hive?createDatabaseIfNotExist=true&lt;/value&gt;</span><br>&lt;/property&gt;<br>#配置登录MySQL的用户名<br>&lt;property&gt;<br>    &lt;name&gt;javax.jdo.option.ConnectionUserName&lt;/name&gt;<br>        &lt;value&gt;root&lt;/value&gt;<br>&lt;/property&gt;<br>#配置登录MySQL的密码<br>&lt;property&gt;<br>    &lt;name&gt;javax.jdo.option.ConnectionPassword&lt;/name&gt;<br>        &lt;value&gt;<span class="hljs-number">123456</span>&lt;/value&gt;<br>&lt;/property&gt;<br>&lt;/configuration&gt;<br></code></pre></td></tr></table></figure><h3 id="6、配置环境变量"><a href="#6、配置环境变量" class="headerlink" title="6、配置环境变量"></a>6、配置环境变量</h3><p>在.bashrc文件中添加内容。先执行以下命令打开.bashrc文件：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs java">vim ~/.bashrc<br></code></pre></td></tr></table></figure><p>在文件末尾加上以下语句：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs java">export HIVE_HOME=/home/hadoop/hive<br>export PATH=$HIVE_HOME/bin:$PATH<br></code></pre></td></tr></table></figure><p>添加完成后保存，然后控制台执行以下命令刷新即可：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs java">source ~/.bashrc<br></code></pre></td></tr></table></figure><h3 id="7、添加MySQL驱动包"><a href="#7、添加MySQL驱动包" class="headerlink" title="7、添加MySQL驱动包"></a>7、添加MySQL驱动包</h3><p>将mysql-connector-java-5.1.38.jar驱动包上传到master主机上，并把它放到hive安装目录下的lib文件夹内即可：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs java">mv ~/mysql-connector-java-<span class="hljs-number">5.1</span><span class="hljs-number">.38</span>.jar ~/hive/lib/<br></code></pre></td></tr></table></figure><h2 id="三、Hive的初始化"><a href="#三、Hive的初始化" class="headerlink" title="三、Hive的初始化"></a>三、Hive的初始化</h2><p>第一次启动Hive需要先进行初始化，后续再次启动则不用。具体操作命令如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs java">schematool -dbType mysql -initSchema<br></code></pre></td></tr></table></figure><p>如果初始化不成功，第二次，第三次的时候要提前删除在mysql的hive数据库，</p><blockquote><p>drop database hive;</p></blockquote><h2 id="四、Hive的启动"><a href="#四、Hive的启动" class="headerlink" title="四、Hive的启动"></a>四、Hive的启动</h2><p>启动Hive时，必须保证Hadoop集群已经正确启动。<br>Hadoop启动后，使用以下命令打开Hive的CLI：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs java">hive<br>hive&gt;show databases;<br></code></pre></td></tr></table></figure><p>如果上述操作没问题，则hive客户端已经安装成功。</p><p>如图：</p><p><img src="https://it.smain.cn/pics/pic_5ea41519.png" alt="pic_5ea41519.png"></p><h2 id="附加：yum仓库换源"><a href="#附加：yum仓库换源" class="headerlink" title="附加：yum仓库换源"></a>附加：yum仓库换源</h2><blockquote><p>如果你的CentOS系统yum命令无法访问默认仓库地址，则可以按照这里的操作步骤进行换源。这里把yum仓库更换为阿里源，以便于国内进行访问。</p></blockquote><p>输入以下命令，使用vim编辑器修改yum配置：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs java">sudo vim /etc/yum.repos.d/CentOS-Base.repo<br></code></pre></td></tr></table></figure><p>然后把里面的内容修改为以下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><code class="hljs java">[base]<br>name=CentOS-$releasever - Base<br>baseurl=http:<span class="hljs-comment">//mirrors.aliyun.com/centos/$releasever/os/$basearch/</span><br>gpgcheck=<span class="hljs-number">1</span><br>gpgkey=http:<span class="hljs-comment">//mirrors.aliyun.com/centos/RPM-GPG-KEY-CentOS-7</span><br><br>#released updates <br>[updates]<br>name=CentOS-$releasever - Updates<br>baseurl=http:<span class="hljs-comment">//mirrors.aliyun.com/centos/$releasever/updates/$basearch/</span><br>gpgcheck=<span class="hljs-number">1</span><br>gpgkey=http:<span class="hljs-comment">//mirrors.aliyun.com/centos/RPM-GPG-KEY-CentOS-7</span><br><br>#additional packages that may be useful<br>[extras]<br>name=CentOS-$releasever - Extras<br>baseurl=http:<span class="hljs-comment">//mirrors.aliyun.com/centos/$releasever/extras/$basearch/</span><br>gpgcheck=<span class="hljs-number">1</span><br>gpgkey=http:<span class="hljs-comment">//mirrors.aliyun.com/centos/RPM-GPG-KEY-CentOS-7</span><br><br>#additional packages that extend functionality of existing packages<br>[centosplus]<br>name=CentOS-$releasever - Plus<br>baseurl=http:<span class="hljs-comment">//mirrors.aliyun.com/centos/$releasever/centosplus/$basearch/</span><br>gpgcheck=<span class="hljs-number">1</span><br>gpgkey=http:<span class="hljs-comment">//mirrors.aliyun.com/centos/RPM-GPG-KEY-CentOS-7</span><br>enabled=<span class="hljs-number">0</span><br></code></pre></td></tr></table></figure><p>修改完成后保存关闭，执行以下命令清理yum的缓存并更新软件包索引：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs java">sudo yum clean all<br>sudo yum makecache<br>sudo wget http:<span class="hljs-comment">//repo.mysql.com/mysql-community-release-el7-5.noarch.rpm</span><br>sudo rpm -ivh mysql-community-release-el7-<span class="hljs-number">5.</span>noarch.rpm<br>sudo yum update<br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>大数据</category>
      
      <category>Hadoop</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Hive</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>【部署】HBase 分布式数据库</title>
    <link href="/2025/06/05/%E3%80%90%E9%83%A8%E7%BD%B2%E3%80%91HBase%20%E5%88%86%E5%B8%83%E5%BC%8F%E6%95%B0%E6%8D%AE%E5%BA%93/"/>
    <url>/2025/06/05/%E3%80%90%E9%83%A8%E7%BD%B2%E3%80%91HBase%20%E5%88%86%E5%B8%83%E5%BC%8F%E6%95%B0%E6%8D%AE%E5%BA%93/</url>
    
    <content type="html"><![CDATA[<h3 id="一、HBase集群的安装与配置"><a href="#一、HBase集群的安装与配置" class="headerlink" title="一、HBase集群的安装与配置"></a>一、HBase集群的安装与配置</h3><p>步骤：</p><h4 id="1、使用XFTP将HBase安装包hbase-1-2-0-bin-tar-gz发送到master机器的主目录。"><a href="#1、使用XFTP将HBase安装包hbase-1-2-0-bin-tar-gz发送到master机器的主目录。" class="headerlink" title="1、使用XFTP将HBase安装包hbase-1.2.0-bin.tar.gz发送到master机器的主目录。"></a>1、使用XFTP将HBase安装包hbase-1.2.0-bin.tar.gz发送到master机器的主目录。</h4><h4 id="2、解压安装包："><a href="#2、解压安装包：" class="headerlink" title="2、解压安装包："></a>2、解压安装包：</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs java">tar -zxvf ~/hbase-<span class="hljs-number">1.2</span><span class="hljs-number">.0</span>-bin.tar.gz<br></code></pre></td></tr></table></figure><h4 id="3、修改文件夹的名字，将其改为hbase，或者创建软连接也可："><a href="#3、修改文件夹的名字，将其改为hbase，或者创建软连接也可：" class="headerlink" title="3、修改文件夹的名字，将其改为hbase，或者创建软连接也可："></a>3、修改文件夹的名字，将其改为hbase，或者创建软连接也可：</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs java">mv ~/hbase-<span class="hljs-number">1.2</span><span class="hljs-number">.0</span> ~/hbase<br></code></pre></td></tr></table></figure><p>以下是配置：</p><h4 id="4、修改HBase集群相关配置文件，这些配置文件位于安装路径下的conf文件夹中，进入到该目录下，修改相关配置文件即可。"><a href="#4、修改HBase集群相关配置文件，这些配置文件位于安装路径下的conf文件夹中，进入到该目录下，修改相关配置文件即可。" class="headerlink" title="4、修改HBase集群相关配置文件，这些配置文件位于安装路径下的conf文件夹中，进入到该目录下，修改相关配置文件即可。"></a>4、修改HBase集群相关配置文件，这些配置文件位于安装路径下的conf文件夹中，进入到该目录下，修改相关配置文件即可。</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs java">cd ~/hbase/conf<br></code></pre></td></tr></table></figure><h4 id="5、先修改hbase-site-xml配置文件："><a href="#5、先修改hbase-site-xml配置文件：" class="headerlink" title="5、先修改hbase-site.xml配置文件："></a>5、先修改hbase-site.xml配置文件：</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs java">vim hbase-site.xml<br></code></pre></td></tr></table></figure><p>内容如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><code class="hljs java">&lt;configuration&gt;<br>&lt;property&gt;<br>&lt;name&gt;hbase.zookeeper.quorum&lt;/name&gt;<br>&lt;value&gt;master,slave1,slave2&lt;/value&gt;<br>&lt;!--指定Zookeeper集群节点--&gt;<br>&lt;/property&gt;<br>&lt;property&gt;<br>      &lt;name&gt;hbase.zookeeper.property.dataDir&lt;/name&gt;<br>      &lt;value&gt;/home/hadoop/data/zookeeper/zkdata&lt;/value&gt;<br>&lt;!--指定Zookeeper数据存储目录--&gt;<br>  &lt;/property&gt;<br>&lt;property&gt;<br>      &lt;name&gt;hbase.zookeeper.property.clientPort&lt;/name&gt;<br>      &lt;value&gt;<span class="hljs-number">2181</span>&lt;/value&gt;<br>&lt;!--指定Zookeeper端口号--&gt;<br>&lt;/property&gt;<br>&lt;property&gt;<br>    &lt;name&gt;hbase.rootdir&lt;/name&gt;<br>    &lt;value&gt;hdfs:<span class="hljs-comment">//mycluster/hbase&lt;/value&gt;</span><br>&lt;!--指定HBase在HDFS上的根目录--&gt;<br>&lt;/property&gt;<br>&lt;property&gt;<br>    &lt;name&gt;hbase.cluster.distributed&lt;/name&gt;<br>    &lt;value&gt;<span class="hljs-literal">true</span>&lt;/value&gt;<br>&lt;!--指定<span class="hljs-literal">true</span>为分布式集群部署--&gt;<br>&lt;/property&gt;<br>&lt;/configuration&gt;<br></code></pre></td></tr></table></figure><p>一些参数，需要根据你的实际情况修改，配置文件修改完成后保存。</p><h4 id="6、修改regionservers配置文件："><a href="#6、修改regionservers配置文件：" class="headerlink" title="6、修改regionservers配置文件："></a>6、修改regionservers配置文件：</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs java">vim regionservers<br></code></pre></td></tr></table></figure><p>配置内容如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs java">master<br>slave1<br>slave2<br></code></pre></td></tr></table></figure><p>添加3个节点角色，三台机器都配置为RegionServer角色；</p><h4 id="7、修改backup-masters配置文件："><a href="#7、修改backup-masters配置文件：" class="headerlink" title="7、修改backup-masters配置文件："></a>7、修改backup-masters配置文件：</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs java">vim backup-masters<br></code></pre></td></tr></table></figure><p>配置内容如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs java">slave1<br></code></pre></td></tr></table></figure><p>为HMaster角色配置高可用，这里选择slave1作为备用节点；</p><h4 id="8、修改hbase-env-sh配置文件，添加相关环境变量："><a href="#8、修改hbase-env-sh配置文件，添加相关环境变量：" class="headerlink" title="8、修改hbase-env.sh配置文件，添加相关环境变量："></a>8、修改hbase-env.sh配置文件，添加相关环境变量：</h4><p>行号显示是 按Esc键输入： :set nu</p><p>（1）配置jdk的路径，找到第27行，先把’#‘去掉，然后把路径修改为jdk的安装路径&#x2F;home&#x2F;hadoop&#x2F;jdk1.8.0_311，该行完整内容为：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs java">vim hbase-env.sh<br></code></pre></td></tr></table></figure><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs java">export JAVA_HOME=/home/hadoop/jdk1<span class="hljs-number">.8</span><span class="hljs-number">.0_311</span>/<br></code></pre></td></tr></table></figure><p>（2）注释掉46、47行，在这两行开始的地方加上’#‘符号；</p><p><img src="https://it.smain.cn/pics/pic_90e8cf46.png" alt="pic_90e8cf46.png"></p><p>（3）不使用内置的ZooKeeper集群，改为使用我们自己的ZooKeeper集群，找到第128行，先把’#‘去掉，然后把该项设置成false，该行完整内容为：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs java">export HBASE_MANAGES_ZK=<span class="hljs-literal">false</span><br></code></pre></td></tr></table></figure><p>以上3步都完成以后，保存。</p><h4 id="9、将配置好的HBase整个文件夹打包发送至slave1和slave2节点，在master上执行以下scp命令："><a href="#9、将配置好的HBase整个文件夹打包发送至slave1和slave2节点，在master上执行以下scp命令：" class="headerlink" title="9、将配置好的HBase整个文件夹打包发送至slave1和slave2节点，在master上执行以下scp命令："></a>9、将配置好的HBase整个文件夹打包发送至slave1和slave2节点，在master上执行以下scp命令：</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs java">scp -r ~/hbase hadoop<span class="hljs-meta">@slave1</span>:~/<br>scp -r ~/hbase hadoop<span class="hljs-meta">@slave2</span>:~/<br></code></pre></td></tr></table></figure><h4 id="10、配置环境变量，后续只需要在master上启动HBase，因此只在master上配置即可"><a href="#10、配置环境变量，后续只需要在master上启动HBase，因此只在master上配置即可" class="headerlink" title="10、配置环境变量，后续只需要在master上启动HBase，因此只在master上配置即可"></a>10、配置环境变量，后续只需要在master上启动HBase，因此只在master上配置即可</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs java">vim ~/.bashrc<br></code></pre></td></tr></table></figure><p>在文件末尾添加以下内容：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs java">export HBASE_HOME=/home/hadoop/hbase<br>export PATH=$HBASE_HOME/bin:$PATH<br></code></pre></td></tr></table></figure><p>保存文件，然后刷新环境变量或重新启动命令行终端：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs java">source ~/.bashrc<br></code></pre></td></tr></table></figure><h3 id="二、HBase集群服务的启动"><a href="#二、HBase集群服务的启动" class="headerlink" title="二、HBase集群服务的启动"></a>二、HBase集群服务的启动</h3><p>HBase集群中的数据是存储在HDFS之中的，而HDFS的高可用集群依赖ZooKeeper提供协调服务，因此要启动HBase集群服务，需要先提前启用ZooKeeper集群，再启动HDFS集群，最后启动HBase集群。</p><p>步骤：</p><h4 id="1、启动ZooKeeper集群，在集群所有节点上分别执行以下命令启动ZooKeeper集群："><a href="#1、启动ZooKeeper集群，在集群所有节点上分别执行以下命令启动ZooKeeper集群：" class="headerlink" title="1、启动ZooKeeper集群，在集群所有节点上分别执行以下命令启动ZooKeeper集群："></a>1、启动ZooKeeper集群，在集群所有节点上分别执行以下命令启动ZooKeeper集群：</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs java">zkServer.sh start<br></code></pre></td></tr></table></figure><h4 id="2、启动HDFS集群，在master上执行以下命令启动HDFS集群："><a href="#2、启动HDFS集群，在master上执行以下命令启动HDFS集群：" class="headerlink" title="2、启动HDFS集群，在master上执行以下命令启动HDFS集群："></a>2、启动HDFS集群，在master上执行以下命令启动HDFS集群：</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs java">start-dfs.sh<br></code></pre></td></tr></table></figure><h4 id="3、启动HBase集群，在master上执行以下命令启动HBase集群："><a href="#3、启动HBase集群，在master上执行以下命令启动HBase集群：" class="headerlink" title="3、启动HBase集群，在master上执行以下命令启动HBase集群："></a>3、启动HBase集群，在master上执行以下命令启动HBase集群：</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs java">start-hbase.sh<br></code></pre></td></tr></table></figure><p>4、使用jps命令可以查看HBase启动进程，如果启动正确，那么在master和slave1上会出现HMaster和HRegionServer两个进程，在slave2上会出现HRegionServer一个进程。</p><p>5、你也可以在浏览器上输入IP:16010端口访问Web界面：<a href="http://192.168.203.133:16010/master-status">http://192.168.203.133:16010/master-status</a></p><p>如果输入master的ip+16010端口，会显示该节点的角色为Master。</p><p>如果输入slave1的ip+16010端口，则会显示该节点角色为Backup Master。</p>]]></content>
    
    
    <categories>
      
      <category>大数据</category>
      
      <category>Hadoop</category>
      
    </categories>
    
    
    <tags>
      
      <tag>HBase</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>【部署】Hadoop 完全分布式系统</title>
    <link href="/2025/06/05/%E3%80%90%E9%83%A8%E7%BD%B2%E3%80%91Haoop%E4%BC%AA%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/"/>
    <url>/2025/06/05/%E3%80%90%E9%83%A8%E7%BD%B2%E3%80%91Haoop%E4%BC%AA%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/</url>
    
    <content type="html"><![CDATA[<h2 id="jdk和hadoop安装包："><a href="#jdk和hadoop安装包：" class="headerlink" title="jdk和hadoop安装包："></a>jdk和hadoop安装包：</h2><h2 id="1、用XFTP发送hadoop安装包和jdk到-home-hadoop-目录下（hadoop用户的主目录）"><a href="#1、用XFTP发送hadoop安装包和jdk到-home-hadoop-目录下（hadoop用户的主目录）" class="headerlink" title="1、用XFTP发送hadoop安装包和jdk到&#x2F;home&#x2F;hadoop&#x2F;目录下（hadoop用户的主目录）"></a>1、用XFTP发送hadoop安装包和jdk到&#x2F;home&#x2F;hadoop&#x2F;目录下（hadoop用户的主目录）</h2><h2 id="2、解压jdk安装包到-目录"><a href="#2、解压jdk安装包到-目录" class="headerlink" title="2、解压jdk安装包到~目录"></a>2、解压jdk安装包到~目录</h2><p>卸载jdk的命令：rpm -qa | grep -i java | xargs -n1 rpm -e –nodeps</p><blockquote><p>cd &#x2F;home&#x2F;hadoop<br>tar -zxvf &#x2F;home&#x2F;hadoop&#x2F;jdk-8u311-linux-x64.tar.gz</p></blockquote><h2 id="3、配置bashrc"><a href="#3、配置bashrc" class="headerlink" title="3、配置bashrc"></a>3、配置bashrc</h2><blockquote><p>vim ~&#x2F;.bashrc</p></blockquote><blockquote><p>export JAVA_HOME&#x3D;&#x2F;home&#x2F;hadoop&#x2F;jdk1.8.0_311<br>export JRE_HOME&#x3D;$JAVA_HOME&#x2F;jre<br>export CLASSPATH&#x3D;$JAVA_HOME&#x2F;lib:$JAVA_HOME&#x2F;jre&#x2F;lib<br>export PATH&#x3D;$PATH:$JAVA_HOME&#x2F;bin:$JRE_HOME</p></blockquote><p>保存后输入source ~&#x2F;.bashrc</p><h2 id="4、继续解压hadoop安装包到-目录-tar-–zxvf-hadoop-2-9-2-tar-gz"><a href="#4、继续解压hadoop安装包到-目录-tar-–zxvf-hadoop-2-9-2-tar-gz" class="headerlink" title="4、继续解压hadoop安装包到~目录   tar –zxvf hadoop-2.9.2.tar.gz"></a>4、继续解压hadoop安装包到~目录   tar –zxvf hadoop-2.9.2.tar.gz</h2><h2 id="5、配置bashrc"><a href="#5、配置bashrc" class="headerlink" title="5、配置bashrc"></a>5、配置bashrc</h2><blockquote><p>vim .bashrc<br>export HADOOP_HOME&#x3D;&#x2F;home&#x2F;hadoop&#x2F;hadoop-2.9.2<br>export PATH&#x3D;$JAVA_HOME&#x2F;bin:$HADOOP_HOME&#x2F;bin:$HADOOP_HOME&#x2F;sbin:$PATH</p></blockquote><blockquote><p>source .bashrc</p></blockquote><h2 id="6、配置Hadoop配置文件，实现伪分布式；"><a href="#6、配置Hadoop配置文件，实现伪分布式；" class="headerlink" title="6、配置Hadoop配置文件，实现伪分布式；"></a>6、配置Hadoop配置文件，实现伪分布式；</h2><blockquote><p>Hadoop 配置文件很多，都位于 $HADOOP_HOME&#x2F;etc&#x2F;hadoop 下。</p><p>下面简单的描述一下几个重要的配置文件：</p><p>hadoop-env.sh：运行 Hadoop 要用的环境变量。</p><p>core-site.xml：核心配置项，包括 HDFS、MapReduce 和 YARN 常用的 I&#x2F;O 设置等。</p><p>hdfs-site.xml：HDFS相关进程的配置项，包括 NameNode、SecondaryNameNode、DataNode等。</p><p>yarn-site.xml：YARN 相关进程的配置项，包括 ResourceManager、NodeManager 等。</p><p>mapred-site.xml：MapReduce 相关进程的配置项。</p><p>slaves：从节点配置文件，通常每行 1 个从节点主机名。</p><p>log4j.properties：系统日志、NameNode 审计日志、JVM 进程日志的配置项。</p></blockquote><h2 id="Hadoop伪分布式配置："><a href="#Hadoop伪分布式配置：" class="headerlink" title="Hadoop伪分布式配置："></a>Hadoop伪分布式配置：</h2><p>所有配置文件都在hadoop安装目录下的&#x2F;etc&#x2F;hadoop&#x2F;里，所以先cd进去：</p><p> cd ~&#x2F;hadoop-2.9.2&#x2F;etc&#x2F;hadoop</p><h3 id="1-vim-hadoop-env-sh"><a href="#1-vim-hadoop-env-sh" class="headerlink" title="1.vim .&#x2F;hadoop-env.sh"></a>1.vim .&#x2F;hadoop-env.sh</h3><p>设置一项java安装目录即可：</p><p>export JAVA_HOME&#x3D;&#x2F;home&#x2F;hadoop&#x2F;jdk1.8.0_311</p><h3 id="2-vim-core-site-xml"><a href="#2-vim-core-site-xml" class="headerlink" title="2.vim .&#x2F;core-site.xml"></a>2.vim .&#x2F;core-site.xml</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs java">&lt;configuration&gt;<br>&lt;property&gt;<br>&lt;name&gt;fs.defaultFS&lt;/name&gt;<br>    &lt;value&gt;hdfs:<span class="hljs-comment">//master:9000&lt;/value&gt;</span><br>&lt;!--配置hdfs NameNode的地址，<span class="hljs-number">9000</span>是RPC通信的端口--&gt;<br>&lt;/property&gt;<br>&lt;property&gt;<br>    &lt;name&gt;hadoop.tmp.dir&lt;/name&gt;<br>    &lt;value&gt;/home/hadoop/data/tmp&lt;/value&gt;<br>&lt;!--hadoop运行时产生的临时文件的存放目录--&gt;<br>&lt;/property&gt;<br>&lt;/configuration&gt;<br></code></pre></td></tr></table></figure><h4 id="3-vim-hdfs-site-xml"><a href="#3-vim-hdfs-site-xml" class="headerlink" title="3.vim hdfs-site.xml"></a>3.vim hdfs-site.xml</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><code class="hljs java">&lt;configuration&gt;<br>&lt;property&gt;<br>&lt;name&gt;dfs.namenode.name.dir&lt;/name&gt;<br>&lt;value&gt;/home/hadoop/data/dfs/name&lt;/value&gt;<br>&lt;!--配置namenode节点存储fsimage的目录位置--&gt;<br>&lt;/property&gt;<br>&lt;property&gt;<br>&lt;name&gt;dfs.datanode.data.dir&lt;/name&gt;<br>&lt;value&gt;/home/hadoop/data/dfs/data&lt;/value&gt;<br>&lt;/property&gt;<br>&lt;property&gt;<br>&lt;name&gt;dfs.replication&lt;/name&gt;<br>&lt;value&gt;<span class="hljs-number">1</span>&lt;/value&gt;<br>&lt;!--配置hdfs副本数量--&gt;<br>&lt;/property&gt;<br>&lt;property&gt;<br>&lt;name&gt;dfs.permissions&lt;/name&gt;<br>&lt;value&gt;<span class="hljs-literal">false</span>&lt;/value&gt;<br>&lt;!--关闭hdfs的权限检查--&gt;<br>&lt;/property&gt;<br>&lt;!--配置datanode 节点存储block的目录位置--&gt;<br>&lt;/configuration&gt;<br></code></pre></td></tr></table></figure><h3 id="4-vim-mapred-site-xml"><a href="#4-vim-mapred-site-xml" class="headerlink" title="4. vim mapred-site.xml"></a>4. vim mapred-site.xml</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs java">&lt;configuration&gt;<br>&lt;property&gt;<br>&lt;name&gt;mapreduce.framework.name&lt;/name&gt;<br>&lt;value&gt;yarn&lt;/value&gt;<br>&lt;!--指定运行mapreduce的环境为YARN--&gt;<br>&lt;/property&gt;<br>&lt;/configuration&gt;<br></code></pre></td></tr></table></figure><h3 id="5-vim-yarn-site-xml"><a href="#5-vim-yarn-site-xml" class="headerlink" title="5.vim yarn-site.xml"></a>5.vim yarn-site.xml</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs java">&lt;configuration&gt;<br>&lt;property&gt;<br>&lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;<br>&lt;value&gt;mapreduce_shuffle&lt;/value&gt;<br>&lt;!--配置NodeManager执行MapReduce任务的方式为Shuffle混洗--&gt;<br>&lt;/property&gt;<br>&lt;/configuration&gt;<br></code></pre></td></tr></table></figure><h3 id="6-vim-slaves"><a href="#6-vim-slaves" class="headerlink" title="6.vim slaves"></a>6.vim slaves</h3><blockquote><p>该文件主要配置datanode角色的主机，目前我们属于伪分布式，因此只填写本机作为从节点即可</p><p>改为：master</p></blockquote><h3 id="7-创建三个用来存放文件"><a href="#7-创建三个用来存放文件" class="headerlink" title="7.创建三个用来存放文件"></a>7.创建三个用来存放文件</h3><blockquote><p>mkdir -p &#x2F;home&#x2F;hadoop&#x2F;data&#x2F;tmp</p><p>mkdir -p &#x2F;home&#x2F;hadoop&#x2F;data&#x2F;dfs&#x2F;name</p><p>mkdir -p &#x2F;home&#x2F;hadoop&#x2F;data&#x2F;dfs&#x2F;data</p></blockquote><h3 id="8-启动Hadoop伪分布式集群并测试："><a href="#8-启动Hadoop伪分布式集群并测试：" class="headerlink" title="8. 启动Hadoop伪分布式集群并测试："></a>8. 启动Hadoop伪分布式集群并测试：</h3><h4 id="（1）格式化NameNode"><a href="#（1）格式化NameNode" class="headerlink" title="（1）格式化NameNode"></a>（1）格式化NameNode</h4><p>hdfs namenode -format</p><h3 id="（2）启动集群"><a href="#（2）启动集群" class="headerlink" title="（2）启动集群"></a>（2）启动集群</h3><p>start-all.sh</p><blockquote><p>查看进程<br>jps<br>查看HDFS<br>浏览器输入网址：<a href="http://master:50070/">http://master:50070/</a></p><p>查看YARN<br>浏览器输入网址：<a href="http://master:8088/">http://master:8088/</a></p><p>测试集群：见课本</p></blockquote>]]></content>
    
    
    <categories>
      
      <category>大数据</category>
      
      <category>Hadoop</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Hadoop</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>【部署】Hadoop 完全分布式系统</title>
    <link href="/2025/06/05/%E3%80%90%E9%83%A8%E7%BD%B2%E3%80%91Hadoop%20%E5%AE%8C%E5%85%A8%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/"/>
    <url>/2025/06/05/%E3%80%90%E9%83%A8%E7%BD%B2%E3%80%91Hadoop%20%E5%AE%8C%E5%85%A8%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/</url>
    
    <content type="html"><![CDATA[<blockquote><p>前言：要顺利完成本次环境搭建，必须准备好三台配置好ZooKeeper集群的虚拟机，再次确保三台主机有独立的IP、独立的主机名、并且配置好IP地址与主机名之间的映射，三台机器之间的SSH免密登录，以及三台机器之间的时间同步。</p><p>我们对于集群配置的修改，可以先在一台机器上进行（如master），修改完成后把配置文件分发到其他机器上即可！！！</p></blockquote><h2 id="本教程的集群的规划如图所示"><a href="#本教程的集群的规划如图所示" class="headerlink" title="本教程的集群的规划如图所示"></a>本教程的集群的规划如图所示</h2><p><img src="/img/hadoop-1.png" alt="规划图"></p><h2 id=""><a href="#" class="headerlink" title=""></a></h2><h2 id="（一）HDFS分布式集群的配置"><a href="#（一）HDFS分布式集群的配置" class="headerlink" title="（一）HDFS分布式集群的配置"></a>（一）HDFS分布式集群的配置</h2><h3 id="一、修改core-site-xml配置文件"><a href="#一、修改core-site-xml配置文件" class="headerlink" title="一、修改core-site.xml配置文件"></a>一、修改core-site.xml配置文件</h3><p>配置文件位于你的hadoop安装文件夹里面的&#x2F;etc&#x2F;hadoop&#x2F;路径下，因此先：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs java">cd ~/hadoop-<span class="hljs-number">2.9</span><span class="hljs-number">.2</span>/etc/hadoop/<br></code></pre></td></tr></table></figure><p>然后再：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs java">vim core-site.xml<br></code></pre></td></tr></table></figure><p>配置内容如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs java">&lt;configuration&gt;<br>&lt;property&gt;<br>&lt;name&gt;fs.defaultFS&lt;/name&gt;<br>&lt;value&gt;hdfs:<span class="hljs-comment">//mycluster&lt;/value&gt;</span><br>&lt;/property&gt;<br>&lt;!--默认的HDFS路径--&gt;<br>&lt;property&gt;<br>&lt;name&gt;hadoop.tmp.dir&lt;/name&gt;<br>&lt;value&gt;/home/hadoop/data/tmp&lt;/value&gt;<br>&lt;/property&gt;<br>&lt;!--hadoop的临时目录，如果需要配置多个目录，需要逗号隔开--&gt;<br>&lt;property&gt;<br>&lt;name&gt;ha.zookeeper.quorum&lt;/name&gt;<br>&lt;value&gt;master:<span class="hljs-number">2181</span>,slave1:<span class="hljs-number">2181</span>,slave2:<span class="hljs-number">2181</span>&lt;/value&gt;<br>&lt;/property&gt;<br>&lt;!--配置Zookeeper管理HDFS--&gt;<br>&lt;/configuration&gt;<br></code></pre></td></tr></table></figure><p>修改完成后，把该配置文件分发到另外两台机器，或者你自行在另外两台机器上修改相应的配置，要确保三台机器配置内容一样。</p><p>以下是发送配置文件的命令，以发送到slave1机器上的为例：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs java">scp -r core-site.xml hadoop<span class="hljs-meta">@slave1</span>:/home/hadoop/hadoop-<span class="hljs-number">2.9</span><span class="hljs-number">.2</span>/etc/hadoop/core-site.xml<br></code></pre></td></tr></table></figure><p>请注意标红的部分，尝试理解这个scp命令，按照实际情况去修改执行。</p><h3 id="二、修改hdfs-site-xml配置文件"><a href="#二、修改hdfs-site-xml配置文件" class="headerlink" title="二、修改hdfs-site.xml配置文件"></a>二、修改hdfs-site.xml配置文件</h3><p>该文件同样位于hadoop安装文件夹里面的&#x2F;etc&#x2F;hadoop&#x2F;路径下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs java">cd ~/hadoop-<span class="hljs-number">2.9</span><span class="hljs-number">.2</span>/etc/hadoop/<br></code></pre></td></tr></table></figure><p>然后再：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs java">vim hdfs-site.xml<br></code></pre></td></tr></table></figure><p>配置内容如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br></pre></td><td class="code"><pre><code class="hljs java">&lt;configuration&gt;<br>  &lt;property&gt;<br>    &lt;name&gt;dfs.replication&lt;/name&gt;<br>    &lt;value&gt;<span class="hljs-number">3</span>&lt;/value&gt;<br>  &lt;/property&gt;<br>  &lt;!--数据块副本数为<span class="hljs-number">3</span>--&gt;<br>  &lt;property&gt;<br>    &lt;name&gt;dfs.permissions&lt;/name&gt;<br>    &lt;value&gt;<span class="hljs-literal">false</span>&lt;/value&gt;<br>  &lt;/property&gt;<br>  &lt;property&gt;<br>    &lt;name&gt;dfs.permissions.enabled&lt;/name&gt;<br>    &lt;value&gt;<span class="hljs-literal">false</span>&lt;/value&gt;<br>  &lt;/property&gt;<br>  &lt;!--权限默认配置为<span class="hljs-literal">false</span>--&gt;<br>  &lt;property&gt;<br>    &lt;name&gt;dfs.nameservices&lt;/name&gt;<br>    &lt;value&gt;mycluster&lt;/value&gt;<br>  &lt;/property&gt;<br>  &lt;!--命名空间，它的值与fs.defaultFS的值要对应，namenode高可用之后有两个namenode，mycluster是对外提供的统一入口--&gt;<br>  &lt;property&gt;<br>    &lt;name&gt;dfs.ha.namenodes.mycluster&lt;/name&gt;<br>    &lt;value&gt;nn1,nn2&lt;/value&gt;<br>  &lt;/property&gt;<br>  &lt;!-- 指定 nameService 是 mycluster时的nameNode有哪些，这里的值也是逻辑名称，名字随便起，相互不重复即可--&gt;<br>  &lt;property&gt;<br>    &lt;name&gt;dfs.namenode.rpc-address.mycluster.nn1&lt;/name&gt;<br>    &lt;value&gt;master:<span class="hljs-number">9000</span>&lt;/value&gt;<br>  &lt;/property&gt;<br>  &lt;property&gt;<br>    &lt;name&gt;dfs.namenode.http-address.mycluster.nn1&lt;/name&gt;<br>    &lt;value&gt;master:<span class="hljs-number">50070</span>&lt;/value&gt;<br>  &lt;/property&gt;<br>  &lt;property&gt;<br>    &lt;name&gt;dfs.namenode.rpc-address.mycluster.nn2&lt;/name&gt;<br>    &lt;value&gt;slave1:<span class="hljs-number">9000</span>&lt;/value&gt;<br>  &lt;/property&gt;<br>  &lt;property&gt;<br>    &lt;name&gt;dfs.namenode.http-address.mycluster.nn2&lt;/name&gt;<br>    &lt;value&gt;slave1:<span class="hljs-number">50070</span>&lt;/value&gt;<br>  &lt;/property&gt;<br>  &lt;property&gt;<br>    &lt;name&gt;dfs.ha.automatic-failover.enabled&lt;/name&gt;<br>    &lt;value&gt;<span class="hljs-literal">true</span>&lt;/value&gt;<br>  &lt;/property&gt;<br>  &lt;!--启动故障自动恢复--&gt;<br>  &lt;property&gt; <br>&lt;name&gt;dfs.namenode.shared.edits.dir&lt;/name&gt;<br>&lt;value&gt;qjournal:<span class="hljs-comment">//master:8485;slave1:8485;slave2:8485/mycluster&lt;/value&gt;</span><br>  &lt;/property&gt;<br>  &lt;!--指定NameNode的元数据在JournalNode上的存放位置--&gt;<br>  &lt;property&gt;<br>    &lt;name&gt;dfs.client.failover.proxy.provider.mycluster&lt;/name&gt;<br>    &lt;value&gt;org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider&lt;/value&gt;<br>  &lt;/property&gt;<br>  &lt;!--指定 mycluster 出故障时，哪个实现类负责执行故障切换--&gt;<br>  &lt;property&gt;<br>    &lt;name&gt;dfs.journalnode.edits.dir&lt;/name&gt;<br>    &lt;value&gt;/home/hadoop/data/journaldata/jn&lt;/value&gt;<br>  &lt;/property&gt;<br>  &lt;!-- 指定JournalNode在本地磁盘存放数据的位置 --&gt;<br>  &lt;property&gt;<br>    &lt;name&gt;dfs.ha.fencing.methods&lt;/name&gt;<br>    &lt;value&gt;shell(/bin/<span class="hljs-literal">true</span>)&lt;/value&gt;<br>  &lt;/property&gt;<br>  &lt;!-- 配置隔离机制,shell通过ssh连接active namenode节点，杀掉进程--&gt;<br>  &lt;property&gt;<br>    &lt;name&gt;dfs.ha.fencing.ssh.<span class="hljs-keyword">private</span>-key-files&lt;/name&gt;<br>    &lt;value&gt;/home/hadoop/.ssh/id_rsa&lt;/value&gt;<br>  &lt;/property&gt;<br>  &lt;!-- 为了实现SSH登录杀掉进程，还需要配置免密码登录的SSH密匙信息 --&gt;<br>  &lt;property&gt;<br>    &lt;name&gt;dfs.ha.fencing.ssh.connect-timeout&lt;/name&gt;<br>    &lt;value&gt;<span class="hljs-number">10000</span>&lt;/value&gt;<br>  &lt;/property&gt;<br>  &lt;property&gt;<br>    &lt;name&gt;dfs.namenode.handler.count&lt;/name&gt;<br>    &lt;value&gt;<span class="hljs-number">100</span>&lt;/value&gt;<br>  &lt;/property&gt;<br>&lt;/configuration&gt;<br></code></pre></td></tr></table></figure><p>请务必认真仔细，注意标红的主机名、路径等信息，要和你实际情况相符。</p><p>修改完成后，把该配置文件分发到另外两台机器，或者你自行在另外两台机器上修改相应的配置，要确保三台机器配置内容一样。</p><p>以下是发送配置文件的命令，以发送到slave1机器上的为例：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs java">scp -r hdfs-site.xml hadoop<span class="hljs-meta">@slave1</span>:/home/hadoop/hadoop-<span class="hljs-number">2.9</span><span class="hljs-number">.2</span>/etc/hadoop/hdfs-site.xml<br></code></pre></td></tr></table></figure><h3 id="三、配置slaves配置文件"><a href="#三、配置slaves配置文件" class="headerlink" title="三、配置slaves配置文件"></a>三、配置slaves配置文件</h3><p>该文件同样位于hadoop安装文件夹里面的&#x2F;etc&#x2F;hadoop&#x2F;路径下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs java">cd ~/hadoop-<span class="hljs-number">2.9</span><span class="hljs-number">.2</span>/etc/hadoop/<br></code></pre></td></tr></table></figure><p>然后再：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs java">vim slaves<br></code></pre></td></tr></table></figure><p>配置内容如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs java">master<br>slave1<br>slave2<br></code></pre></td></tr></table></figure><p>以下是发送配置文件的命令，以发送到slave1机器上的为例：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs java">scp -r slaves hadoop<span class="hljs-meta">@slave1</span>:/home/hadoop/hadoop-<span class="hljs-number">2.9</span><span class="hljs-number">.2</span>/etc/hadoop/slaves<br></code></pre></td></tr></table></figure><h3 id="四、启动HDFS集群服务"><a href="#四、启动HDFS集群服务" class="headerlink" title="四、启动HDFS集群服务"></a>四、启动HDFS集群服务</h3><p>步骤：</p><p>1、HDFS的高可用实现依赖ZooKeeper，因此先启动三台机器的ZooKeeper服务；</p><p>2、分别启动JournalNode集群，在三台机器上各自执行以下命令：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs java">hadoop-daemon.sh start journalnode<br></code></pre></td></tr></table></figure><p>3、格式化主节点NameNode，在master上，执行以下命令：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs java">hdfs namenode -format                   <span class="hljs-comment">// 格式化NameNode</span><br>hdfs zkfc -formatZK                     <span class="hljs-comment">// 格式化ZooKeeper高可用</span><br>hdfs namenode                            <span class="hljs-comment">// 启动NameNode</span><br></code></pre></td></tr></table></figure><p>4、master上启动NameNode进程后，在slave1上就可以同步NameNode元数据，在slave1上执行以下命令：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs java">hdfs namenode -bootstrapStandby<br></code></pre></td></tr></table></figure><p>5、第4步同步完成后，可以关闭master上的NameNode进程，切回到master上的终端，使用&lt;Ctrl+C&gt;组合键结束进程即可。</p><p>6、关闭JournalNode集群，在三台机器上各自执行以下命令：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs java">hadoop-daemon.sh stop journalnode<br></code></pre></td></tr></table></figure><p>如果上述步骤没有报错，则后续可以在master上一键启动HDFS集群：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs java">start-dfs.sh<br></code></pre></td></tr></table></figure><p>测试：启动成功后，可以通过master、slave1的IP地址+50070端口在浏览器端访问HDFS集群的Web界面，查看状态，如：<a href="http://192.168.203.133:50070，或者使用以下命令查看NameNode状态：">http://192.168.203.133:50070，或者使用以下命令查看NameNode状态：</a></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs java">hdfs haadmin -getServiceState nn1<br>hdfs haadmin -getServiceState nn2<br></code></pre></td></tr></table></figure><p>测试2：可以尝试使用hadoop fs -mkdir命令创建一个&#x2F;test文件夹，然后再把words.log文件上传到HDFS集群中，检查HDFS能否正常使用。</p><h2 id="（二）YARN分布式集群的配置"><a href="#（二）YARN分布式集群的配置" class="headerlink" title="（二）YARN分布式集群的配置"></a>（二）YARN分布式集群的配置</h2><h3 id="一、修改mapred-site-xml配置文件"><a href="#一、修改mapred-site-xml配置文件" class="headerlink" title="一、修改mapred-site.xml配置文件"></a>一、修改mapred-site.xml配置文件</h3><p>该文件主要配置MapReduce相关属性，将MapReduce运行环境指定未YARN即可，这里和前面的配置中是一致的，不需修改。文件内容如下，可供确认：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs java">&lt;configuration&gt;<br>&lt;property&gt;<br>&lt;name&gt;mapreduce.framework.name&lt;/name&gt;<br>&lt;value&gt;yarn&lt;/value&gt;<br>&lt;/property&gt;<br>&lt;!--指定运行MapReduce的环境是YARN--&gt;<br>&lt;/configuration&gt;<br></code></pre></td></tr></table></figure><h3 id="二、修改yarn-site-xml配置文件"><a href="#二、修改yarn-site-xml配置文件" class="headerlink" title="二、修改yarn-site.xml配置文件"></a>二、修改yarn-site.xml配置文件</h3><p>该文件同样位于hadoop安装文件夹里面的&#x2F;etc&#x2F;hadoop&#x2F;路径下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs java">cd ~/hadoop-<span class="hljs-number">2.9</span><span class="hljs-number">.2</span>/etc/hadoop/<br></code></pre></td></tr></table></figure><p>然后再：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs java">vim yarn-site.xml<br></code></pre></td></tr></table></figure><p>配置内容如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br></pre></td><td class="code"><pre><code class="hljs java">&lt;configuration&gt;<br>  &lt;property&gt;<br>    &lt;name&gt;yarn.resourcemanager.connect.retry-interval.ms&lt;/name&gt;<br>    &lt;value&gt;<span class="hljs-number">2000</span>&lt;/value&gt;<br>  &lt;/property&gt;<br>  &lt;property&gt;<br>    &lt;name&gt;yarn.resourcemanager.ha.enabled&lt;/name&gt;<br>    &lt;value&gt;<span class="hljs-literal">true</span>&lt;/value&gt;<br>  &lt;/property&gt;<br>  &lt;!--打开高可用--&gt;<br>  &lt;property&gt;<br>    &lt;name&gt;yarn.resourcemanager.ha.automatic-failover.enabled&lt;/name&gt;<br>    &lt;value&gt;<span class="hljs-literal">true</span>&lt;/value&gt;<br>  &lt;/property&gt;<br>  &lt;!--启动故障自动恢复--&gt;<br>  &lt;property&gt;<br>    &lt;name&gt;yarn.resourcemanager.ha.automatic-failover.embedded&lt;/name&gt;<br>    &lt;value&gt;<span class="hljs-literal">true</span>&lt;/value&gt;<br>  &lt;/property&gt;<br>  &lt;!--RM启动内置选举active--&gt;<br>  &lt;property&gt;<br>    &lt;name&gt;yarn.resourcemanager.cluster-id&lt;/name&gt;<br>    &lt;value&gt;yarn-rm-cluster&lt;/value&gt;<br>  &lt;/property&gt;<br>  &lt;!--给yarn cluster 取个名字yarn-rm-cluster--&gt;<br>  &lt;property&gt;<br>    &lt;name&gt;yarn.resourcemanager.ha.rm-ids&lt;/name&gt;<br>    &lt;value&gt;rm1,rm2&lt;/value&gt;<br>  &lt;/property&gt;<br>  &lt;!--ResourceManager高可用 rm1,rm2--&gt;<br>  &lt;property&gt;<br>    &lt;name&gt;yarn.resourcemanager.hostname.rm1&lt;/name&gt;<br>    &lt;value&gt;master&lt;/value&gt;<br>  &lt;/property&gt;<br>  &lt;property&gt;<br>    &lt;name&gt;yarn.resourcemanager.hostname.rm2&lt;/name&gt;<br>    &lt;value&gt;slave1&lt;/value&gt;<br>  &lt;/property&gt;<br>  &lt;property&gt;<br>    &lt;name&gt;yarn.resourcemanager.recovery.enabled&lt;/name&gt;<br>    &lt;value&gt;<span class="hljs-literal">true</span>&lt;/value&gt;<br>&lt;/property&gt;<br>  &lt;!--启用resourcemanager 自动恢复--&gt;<br>  &lt;property&gt;<br>    &lt;name&gt;hadoop.zk.address&lt;/name&gt;<br>    &lt;value&gt;master:<span class="hljs-number">2181</span>,slave1:<span class="hljs-number">2181</span>,slave2:<span class="hljs-number">2181</span>&lt;/value&gt;<br>  &lt;/property&gt;<br>  &lt;!--配置Zookeeper地址作为状态存储和leader选举--&gt;<br>  &lt;property&gt;<br>    &lt;name&gt;yarn.resourcemanager.address.rm1&lt;/name&gt;<br>    &lt;value&gt;master:<span class="hljs-number">8032</span>&lt;/value&gt;<br>  &lt;/property&gt;<br>  &lt;!--rm1端口号--&gt;<br>  &lt;property&gt;<br>    &lt;name&gt;yarn.resourcemanager.scheduler.address.rm1&lt;/name&gt;<br>    &lt;value&gt;master:<span class="hljs-number">8034</span>&lt;/value&gt;<br>  &lt;/property&gt;<br>  &lt;!-- rm1调度器的端口号--&gt;<br>  &lt;property&gt;<br>    &lt;name&gt;yarn.resourcemanager.webapp.address.rm1&lt;/name&gt;<br>    &lt;value&gt; master:<span class="hljs-number">8088</span>&lt;/value&gt;<br>  &lt;/property&gt;<br>  &lt;!-- rm1 webapp端口号--&gt;<br>  &lt;property&gt;<br>    &lt;name&gt;yarn.resourcemanager.address.rm2&lt;/name&gt;<br>    &lt;value&gt;slave1:<span class="hljs-number">8032</span>&lt;/value&gt;<br>  &lt;/property&gt;<br>  &lt;property&gt;<br>    &lt;name&gt;yarn.resourcemanager.scheduler.address.rm2&lt;/name&gt;<br>    &lt;value&gt;slave1:<span class="hljs-number">8034</span>&lt;/value&gt;<br>  &lt;/property&gt;<br>  &lt;property&gt;<br>    &lt;name&gt;yarn.resourcemanager.webapp.address.rm2&lt;/name&gt;<br>    &lt;value&gt;slave1:<span class="hljs-number">8088</span>&lt;/value&gt;<br>  &lt;/property&gt;<br>  &lt;property&gt;<br>    &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;<br>    &lt;value&gt;mapreduce_shuffle&lt;/value&gt;<br>  &lt;/property&gt;<br>  &lt;property&gt;<br>    &lt;name&gt;yarn.nodemanager.aux-services.mapreduce_shuffle.class&lt;/name&gt;<br>    &lt;value&gt;org.apache.hadoop.mapred.ShuffleHandler&lt;/value&gt;<br>  &lt;/property&gt;<br>  &lt;!--执行MapReduce需要配置的shuffle过程--&gt;<br>  &lt;property&gt;<br>    &lt;name&gt;yarn.nodemanager.vmem-check-enabled&lt;/name&gt;<br>    &lt;value&gt;<span class="hljs-literal">false</span>&lt;/value&gt;<br>  &lt;/property&gt;<br>  &lt;property&gt;<br>    &lt;name&gt;yarn.nodemanager.vmem-pmem-ratio&lt;/name&gt;<br> &lt;value&gt;<span class="hljs-number">4</span>&lt;/value&gt;<br>  &lt;/property&gt;<br>&lt;/configuration&gt;<br></code></pre></td></tr></table></figure><p>同样务必认真仔细，注意标红的主机名信息，要和你实际情况相符。</p><p>修改完成后，把该配置文件分发到另外两台机器，或者你自行在另外两台机器上修改相应的配置，要确保三台机器配置内容一样。</p><p>以下是发送配置文件的命令，以发送到slave1机器上的为例：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs java">scp -r yarn-site.xml hadoop<span class="hljs-meta">@slave1</span>:/home/hadoop/hadoop-<span class="hljs-number">2.9</span><span class="hljs-number">.2</span>/etc/hadoop/yarn-site.xml<br></code></pre></td></tr></table></figure><h3 id="三、启动YARN集群服务"><a href="#三、启动YARN集群服务" class="headerlink" title="三、启动YARN集群服务"></a>三、启动YARN集群服务</h3><p>步骤：</p><p>1、YARN的高可用实现也依赖与ZooKeeper，但前面已经启动了ZooKeeper服务，因此这步可以跳过。</p><p>2、一键启动YARN集群，在master上执行脚本命令：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs java">start-yarn.sh<br></code></pre></td></tr></table></figure><p>3、由于上面脚本不包含备用ResourceManager的启动，因此要手动启动备用ResourceManager，在slave1上执行命令：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs java">yarn-daemon.sh start resourcemanager<br></code></pre></td></tr></table></figure><p>4、测试：启动成功后，可以通过master、slave1的IP地址+8088端口在浏览器端访问YARN集群的Web界面，也可以使用以下命令查看两个ResourceManager的状态：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs java">yarn rmadmin -getServiceState rm1<br>yarn rmadmin -getServiceState rm2<br></code></pre></td></tr></table></figure><p>5、测试2：尝试上传一些数据集（用之前的天气数据，或者words.log都行）到HDFS中，然后执行一下对应的MapReduce程序，看看能否正常运行。</p>]]></content>
    
    
    <categories>
      
      <category>大数据</category>
      
      <category>Hadoop</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Hadoop</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>【部署】Flume 日志采集系统</title>
    <link href="/2025/06/05/%E3%80%90%E9%83%A8%E7%BD%B2%E3%80%91Flume%20%E6%97%A5%E5%BF%97%E9%87%87%E9%9B%86%E7%B3%BB%E7%BB%9F/"/>
    <url>/2025/06/05/%E3%80%90%E9%83%A8%E7%BD%B2%E3%80%91Flume%20%E6%97%A5%E5%BF%97%E9%87%87%E9%9B%86%E7%B3%BB%E7%BB%9F/</url>
    
    <content type="html"><![CDATA[<h2 id="一、Flume安装与配置"><a href="#一、Flume安装与配置" class="headerlink" title="一、Flume安装与配置"></a>一、Flume安装与配置</h2><p>步骤：</p><p>1、使用XFTP将Flume安装包apache-flume-1.9.0-bin.tar.gz发送到master机器的主目录。</p><p>2、解压安装包：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs java">tar -zxvf ~/apache-flume-<span class="hljs-number">1.9</span><span class="hljs-number">.0</span>-bin.tar.gz<br></code></pre></td></tr></table></figure><p>3、修改文件夹的名字，将其改为flume，或者创建软连接也可：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs java">mv ~/apache-flume-<span class="hljs-number">1.9</span><span class="hljs-number">.0</span>-bin ~/flume<br></code></pre></td></tr></table></figure><p>4、配置环境变量：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs java">vim ~/.bashrc<br></code></pre></td></tr></table></figure><p>在文件末尾添加以下内容：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs java">export FLUME_HOME=/home/hadoop/flume<br>export PATH=$FLUME_HOME/bin:$PATH<br></code></pre></td></tr></table></figure><p>保存文件，然后刷新环境变量或重新启动命令行终端：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs java">source ~/.bashrc<br></code></pre></td></tr></table></figure><h2 id=""><a href="#" class="headerlink" title=""></a></h2><h2 id="二、测试运行"><a href="#二、测试运行" class="headerlink" title="二、测试运行"></a>二、测试运行</h2><p>可以直接使用以下Flume的默认配置启动Agent，该Agent的Source是一个序列生成器，Channel是内存，Sink是日志类型，直接打印到控制台。</p><p>Flume的配置可以在任意地方编写，只需在执行启动命令时，指定该配置即可。</p><p>步骤：</p><p>1、使用mv命令更改Flume自带的配置文件模版文件名：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs java">cd ~/flume/conf<br>vim net-flume-logger.conf<br></code></pre></td></tr></table></figure><p>配置内容如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs java"># 把这个agent命名为a1，且定义了source、sink、channel<br>a1.sources = r1<br>a1.sinks = k1<br>a1.channels = c1<br># 配置source<br>a1.sources.r1.type = netcat<br>a1.sources.r1.bind = localhost<br>a1.sources.r1.port = <span class="hljs-number">44444</span><br># 配置sink<br>a1.sinks.k1.type = logger<br># 使用内存作为Channel<br>a1.channels.c1.type = memory<br>a1.channels.c1.capacity = <span class="hljs-number">1000</span><br>a1.channels.c1.transactionCapacity = <span class="hljs-number">100</span><br># 绑定source和sink的channel<br>a1.sources.r1.channels = c1<br>a1.sinks.k1.channel = c1<br></code></pre></td></tr></table></figure><p>2、启动Flume Agent：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs java">flume-ng agent -n a1 -c ~/flume/conf -f ~/flume/conf/net-flume-logger.conf -Dflume.root.logger=INFO,console<br></code></pre></td></tr></table></figure><p>3、接着另外打开一个终端，使用以下命令，往44444端口发送消息：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs java">nc localhost <span class="hljs-number">44444</span><br></code></pre></td></tr></table></figure><p>在Flume Agent能看到对应的Event，则Flume能够正确运行。</p><p><img src="https://it.smain.cn/pics/pic_96deec32.png" alt="pic_96deec32.png"></p><p><img src="https://it.smain.cn/pics/pic_32719d41.png" alt="pic_32719d41.png"></p><p>4、接着可以为其他机器都部署好Flume，使用scp -r命令把文件夹发送到另外两台机器，然后配置环境变量即可。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs java">scp -r ~/flume hadoop<span class="hljs-meta">@slave1</span>:~/<br>scp -r ~/flume hadoop<span class="hljs-meta">@slave2</span>:~/<br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>大数据</category>
      
      <category>Hadoop</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Flume</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>【部署】Flink 流式处理框架</title>
    <link href="/2025/06/05/%E3%80%90%E9%83%A8%E7%BD%B2%E3%80%91Flink%20%E6%B5%81%E5%BC%8F%E5%A4%84%E7%90%86%E6%A1%86%E6%9E%B6/"/>
    <url>/2025/06/05/%E3%80%90%E9%83%A8%E7%BD%B2%E3%80%91Flink%20%E6%B5%81%E5%BC%8F%E5%A4%84%E7%90%86%E6%A1%86%E6%9E%B6/</url>
    
    <content type="html"><![CDATA[<h3 id="一、单机模式"><a href="#一、单机模式" class="headerlink" title="一、单机模式"></a>一、单机模式</h3><h4 id="步骤："><a href="#步骤：" class="headerlink" title="步骤："></a>步骤：</h4><p>1、使用XFTP将Flink安装包flink-1.13.5-bin-scala_2.11.tgz发送到master机器的主目录。</p><p>2、解压安装包：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs java">tar -zxvf ~/flink-<span class="hljs-number">1.13</span><span class="hljs-number">.5</span>-bin-scala_2<span class="hljs-number">.11</span>.tgz<br></code></pre></td></tr></table></figure><p>3、修改文件夹的名字，将其改为flume，或者创建软连接也可：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs java">mv ~/flink-<span class="hljs-number">1.13</span><span class="hljs-number">.5</span> ~/flink<br></code></pre></td></tr></table></figure><p>4、配置环境变量：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs java">vim ~/.bashrc<br></code></pre></td></tr></table></figure><p>然后在文件末尾添加以下内容：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs java">export FLINK_HOME=/home/hadoop/flink<br><br>export PATH=$FLINK_HOME/bin:$PATH<br></code></pre></td></tr></table></figure><p>保存文件，然后刷新环境变量或重新启动命令行终端：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs java">source ~/.bashrc<br></code></pre></td></tr></table></figure><p>5、可以使用以下命令开启一个Flink的Scala控制台：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs java">start-scala-shell.sh local<br></code></pre></td></tr></table></figure><h3 id="二、Flink-Standalone模式集群"><a href="#二、Flink-Standalone模式集群" class="headerlink" title="二、Flink Standalone模式集群"></a>二、Flink Standalone模式集群</h3><h4 id="步骤：-1"><a href="#步骤：-1" class="headerlink" title="步骤："></a>步骤：</h4><p>1、使用vim命令配置Flink的flink-conf.yaml配置文件：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs java">cd ~/flink/conf<br></code></pre></td></tr></table></figure><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs java">vim flink-conf.yaml<br></code></pre></td></tr></table></figure><p>配置内容如下（注意：这里仅展示其中要修改的参数，其余参数配置保持不变）：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><code class="hljs java">jobmanager.rpc.address: master<br><br>jobmanager.rpc.port: <span class="hljs-number">6123</span><br><br>jobmanager.memory.process.size: 1600m<br><br>taskmanager.memory.process.size: 1728m<br><br>taskmanager.numberOfTaskSlots: <span class="hljs-number">3</span><br><br>parallelism.<span class="hljs-keyword">default</span>: <span class="hljs-number">3</span><br><br>high-availability: zookeeper<br><br>high-availability.zookeeper.quorum: master:<span class="hljs-number">2181</span>,slave1:<span class="hljs-number">2181</span>,slave2:<span class="hljs-number">2181</span><br><br>high-availability.cluster-id: /cluster_one<br><br>high-availability.zookeeper.path.root: /flink<br><br>high-availability.storageDir: hdfs:<span class="hljs-comment">//mycluster/flink/ha/</span><br></code></pre></td></tr></table></figure><p>2、配置workers：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs java">vim workers<br></code></pre></td></tr></table></figure><p>修改为以下内容：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs java">master<br>slave1<br>slave2<br></code></pre></td></tr></table></figure><p>3、配置masters，为节省资源，我们只在master启动主节点即可，如需启动备用节点，则需要在此处添加其他节点以作为备用节点：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs java">vim masters<br></code></pre></td></tr></table></figure><p>修改为以下内容：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs java">master:<span class="hljs-number">8088</span><br></code></pre></td></tr></table></figure><p>可指定端口，可访问web</p><p>4、添加Hadoop依赖包，若打算使Flink与Hadoop能够协同工作，比如后续在YARN上运行Flink、使用Flink连接到HDFS、HBase等，那么就需要将Flink对应Hadoop版本的shaded包，放到Flink安装目录的lib文件夹内，或者添加到Hadoop的环境变量中，这里使用第一种方法进行配置：</p><p>使用XFTP将 flink-shaded-hadoop-2-uber-2.8.3-10.0.jar 放到 ~&#x2F;flink&#x2F;lib 文件夹内</p><p>5、接着把配置好的Flink安装目录使用scp命令发送到其他节点：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs java">scp -r ~/flink hadoop<span class="hljs-meta">@slave1</span>:~<br><br>scp -r ~/flink hadoop<span class="hljs-meta">@slave2</span>:~<br></code></pre></td></tr></table></figure><p>6、（如第3步中配置了其他备用主节点，则需要做这步，只配置了一个master的话，这一步可以跳过）在备用主节点slave1上修改flink-conf.yaml，将其中的jobmanager.rpc.address一项修改为</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs java">slave1：<br><br>cd ~/flink/conf<br><br>vim flink-conf.yaml<br></code></pre></td></tr></table></figure><p>修改内容如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs java">jobmanager.rpc.address: slave1<br></code></pre></td></tr></table></figure><p>7、依次启动ZooKeeper、HDFS、YARN集群，以启动Hadoop；</p><p>8、在master节点上，启动Flink Standalone集群：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs java">start-cluster.sh<br></code></pre></td></tr></table></figure><h3 id="三、Flink-on-YARN模式"><a href="#三、Flink-on-YARN模式" class="headerlink" title="三、Flink on YARN模式"></a>三、Flink on YARN模式</h3><p>要使用Flink on YARN只需要修改环境变量，让Flink可以从系统环境变量中读取到Hadoop的配置信息即可：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs java">vim ~/.bashrc<br></code></pre></td></tr></table></figure><p>然后在末尾添加Hadoop配置文件夹的目录环境：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs java">export HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop<br></code></pre></td></tr></table></figure><p>然后刷新环境变量：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs java">source ~/.bashrc<br></code></pre></td></tr></table></figure><p>后续就可以使用 yarn-session.sh 启动 yarn session 来运行Flink任务，或者使用 flink run 命令直接将Flink任务提交到YARN上运行。</p>]]></content>
    
    
    <categories>
      
      <category>大数据</category>
      
      <category>Hadoop</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Flink</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>【部署】Davinci 可视化工具</title>
    <link href="/2025/06/05/%E3%80%90%E9%83%A8%E7%BD%B2%E3%80%91Davinci%20%E5%8F%AF%E8%A7%86%E5%8C%96%E5%B7%A5%E5%85%B7/"/>
    <url>/2025/06/05/%E3%80%90%E9%83%A8%E7%BD%B2%E3%80%91Davinci%20%E5%8F%AF%E8%A7%86%E5%8C%96%E5%B7%A5%E5%85%B7/</url>
    
    <content type="html"><![CDATA[<blockquote><p>Davinci作为数据可视化工具，只需要安装到一个节点上即可</p></blockquote><h3 id="一、Davinci安装与配置"><a href="#一、Davinci安装与配置" class="headerlink" title="一、Davinci安装与配置"></a>一、Davinci安装与配置</h3><h4 id="步骤："><a href="#步骤：" class="headerlink" title="步骤："></a>步骤：</h4><p>1.使用XFTP将Phantomjs和Davinci的安装包phantomjs-2.1.1-linux-x86_64.tar.bz2以及davinci-assembly_3.0.1-0.3.1-SNAPSHOT-dist-beta.9.zip发送到master机器的主目录。</p><p>2、先安装Phantomjs：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs java">tar -jxvf ~/phantomjs-<span class="hljs-number">2.1</span><span class="hljs-number">.1</span>-linux-x86_64.tar.bz2<br></code></pre></td></tr></table></figure><p>然后修改文件夹的名字，将其改为phantomjs，或者创建软连接也可：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs java">mv ~/phantomjs-<span class="hljs-number">2.1</span><span class="hljs-number">.1</span>-linux-x86_64 ~/phantomjs<br></code></pre></td></tr></table></figure><p>3、安装Davinci，由于它是zip压缩包，因此解压方式稍有不同：</p><p>先创建一个davinci文件夹：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs java">mkdir ~/davinci<br></code></pre></td></tr></table></figure><p>然后将davinci的压缩包移动到davinci文件夹里面，再进行解压：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs java">mv ~/davinci-assembly_3<span class="hljs-number">.0</span><span class="hljs-number">.1</span>-<span class="hljs-number">0.3</span><span class="hljs-number">.1</span>-SNAPSHOT-dist-beta<span class="hljs-number">.9</span>.zip ~/davinci<br><br>cd ~/davinci<br><br>unzip davinci-assembly_3<span class="hljs-number">.0</span><span class="hljs-number">.1</span>-<span class="hljs-number">0.3</span><span class="hljs-number">.1</span>-SNAPSHOT-dist-beta<span class="hljs-number">.9</span>.zip<br></code></pre></td></tr></table></figure><p>4、配置环境变量：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs java">vim ~/.bashrc<br></code></pre></td></tr></table></figure><p>在文件末尾添加以下内容：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs java">export DAVINCI3_HOME=/home/hadoop/davinci<br><br>export PATH=$DAVINCI3_HOME/bin:$PATH<br></code></pre></td></tr></table></figure><p>保存文件，然后刷新环境变量或重新启动命令行终端：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs java">source ~/.bashrc<br></code></pre></td></tr></table></figure><p>5、进到MySQL，创建davinci数据库，让Davinci能够使用MySQL存储状态信息：</p><blockquote><p>mysql -u root -p123456</p><p>mysql&gt; CREATE DATABASE IF NOT EXISTS davinci DEFAULT CHARSET utf8mb4 COLLATE utf8mb4_general_ci;</p></blockquote><p>6、授予当前节点root用户权限，以及授予root用户远程访问权限（若在Sqoop安装部署时做过，这步可以省略），在MySQL里面继续执行：</p><blockquote><p>mysql&gt; grant all on *.* to ‘root‘@’master’ identified by ‘123456’ with grant option;</p><p>mysql&gt; grant all on *.* to ‘root‘@’%’ identified by ‘123456’ with grant option;</p><p>mysql&gt; flush privileges;</p></blockquote><p>7、退出MySQL，修改Davinci的初始化脚本：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs java">cd ~/davinci/bin<br><br>vim initdb.sh<br></code></pre></td></tr></table></figure><p>将里面的连接配置修改成以下，主要改掉标红的参数即可：</p><blockquote><p>mysql -P 3306 -h master -u root -p123456 davinci &lt; $DAVINCI3_HOME&#x2F;bin&#x2F;davinci.sql</p></blockquote><p>修改完以后保存。</p><p>8、增加执行权限，然后执行脚本：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs java">chmod u+x ~/davinci/bin/initdb.sh<br><br>sh ~/davinci/bin/initdb.sh<br></code></pre></td></tr></table></figure><p>9、修改Davinci的服务端配置文件application.yml，先从模板复制过来，然后找到对应的地方修改参数就行：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs java">cd ~/davinci/config<br><br>mv application.yml.example application.yml<br><br>vim application.yml<br></code></pre></td></tr></table></figure><p>请一定要小心配置，该文件是yml配置文件，对缩进是敏感的，不能随意删减空格：</p><p>①、server项，先找到以下片段，主要修改其中的地址项：（修改，不要复制我的，复制会导致缩进不一致）</p><blockquote><p>server:</p><p>protocol: http</p><p>address: master</p><p>port: 8080</p></blockquote><p>②、datasource项，同样找到以下片段，修改其中的参数，这里是连接MySQL的参数配置：</p><blockquote><p>datasource:</p><p>url: jdbc:mysql:&#x2F;&#x2F;master:3306&#x2F;davinci?useUnicode&#x3D;true&amp;characterEncoding&#x3D;UTF-8&amp;zeroDateTimeBehavior&#x3D;convertToNull&amp;allowMultiQueries&#x3D;true</p><p>username: root</p><p>password: 123456</p><p>driver-class-name: com.mysql.jdbc.Driver</p></blockquote><p>③、mail项（十分重要），你需要一个自己的邮箱，并且启动该邮箱的SMTP服务，这里以QQ邮箱为例来配置：（请注意每个参数冒号后面都有1个空格）</p><blockquote><p>mail:</p><p>host: smtp.qq.com</p><p>port: 465</p><p>username: <a href="mailto:&#55;&#x39;&#56;&#x33;&#x31;&#50;&#49;&#x37;&#57;&#x40;&#113;&#113;&#46;&#x63;&#x6f;&#109;">&#55;&#x39;&#56;&#x33;&#x31;&#50;&#49;&#x37;&#57;&#x40;&#113;&#113;&#46;&#x63;&#x6f;&#109;</a> （此处你不能复制我的）</p><p>fromAddress: <a href="mailto:&#55;&#x39;&#56;&#x33;&#x31;&#x32;&#x31;&#x37;&#x39;&#x40;&#113;&#x71;&#46;&#x63;&#111;&#109;">&#55;&#x39;&#56;&#x33;&#x31;&#x32;&#x31;&#x37;&#x39;&#x40;&#113;&#x71;&#46;&#x63;&#111;&#109;</a> （此处你不能复制我的）</p><p>password: hrqd********bcja （此处你不能复制我的）</p><p>nickname: Davinci</p><p>properties:</p><p> mail.smtp.auth: true</p><p> mail.smtp.starttls.enable: false</p><p> mail.smtp.ssl.enable: true</p></blockquote><p>其中properties项，整个重构成上面配置里的3行即可，properties要对齐到上面nickname这个项的缩进，然后里面的子项则需要再缩进2空格。</p><p>这里的password是你邮箱STMP服务的授权码，或者是登录密码，但QQ邮箱是不允许使用STMP服务用密码去登录QQ邮箱的，因此要在个人QQ邮箱的安全管理页面，生成一个授权码，然后让Davinci这个授权码去登录邮箱发送邮件。</p><p>参考地址：<a href="https://wx.mail.qq.com/list/readtemplate?name=app_intro.html#/agreement/authorizationCode">https://wx.mail.qq.com/list/readtemplate?name=app_intro.html#&#x2F;agreement&#x2F;authorizationCode</a></p><p>QQ邮箱需要在账号与安全-安全设置里面开启STMP服务，同时生成授权码即可，然后将该授权码填入上面配置信息里的password一项。</p><p>其他邮箱请自行研究相关STMP服务的开启办法以及授权码的获取办法。</p><p>④、screenshot项：</p><blockquote><p>screenshot:</p><p>default_browser: PHANTOMJS</p><p>timeout_second: 600</p><p>phantomjs_path: &#x2F;home&#x2F;hadoop&#x2F;phantomjs</p><p>chromedriver_path: $your_chromedriver_path$</p></blockquote><h3 id="二、测试运行"><a href="#二、测试运行" class="headerlink" title="二、测试运行"></a>二、测试运行</h3><h4 id="步骤：-1"><a href="#步骤：-1" class="headerlink" title="步骤："></a>步骤：</h4><p>1、启动Davinci服务端：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs java">start-server.sh<br></code></pre></td></tr></table></figure><p>2、没有报错的话，在浏览器中以master机器的IP地址加8080端口，进入Davinci的登录页面：</p><p><a href="http://192.168.203.128:8080/">http://192.168.203.128:8080/</a></p><p>能进入到登录界面，则服务端启动成功。</p><p>3、然后本地化部署完Davinci以后，第一次使用需要注册账号，要用一个可以接收邮件的邮箱来注册，点击注册以后，邮箱会收到一封激活邮件，在本机点击激活即可完成注册。</p><p>若是点激活没反应，可以使用右键保存链接地址，然后在新的选项卡里面直接粘贴地址并打开即可。</p><p>后续可以使用该账户登录Davinci。</p>]]></content>
    
    
    <categories>
      
      <category>大数据</category>
      
      <category>Hadoop</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Davinci</tag>
      
      <tag>可视化</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>免费容器部署平台ClawCloud</title>
    <link href="/2025/06/03/%E5%85%8D%E8%B4%B9%E5%AE%B9%E5%99%A8%E9%83%A8%E7%BD%B2%E5%B9%B3%E5%8F%B0ClawCloud/"/>
    <url>/2025/06/03/%E5%85%8D%E8%B4%B9%E5%AE%B9%E5%99%A8%E9%83%A8%E7%BD%B2%E5%B9%B3%E5%8F%B0ClawCloud/</url>
    
    <content type="html"><![CDATA[<!-- 玩转AI客户端，工作流，alist， 数据库 --><blockquote><p>官网地址：<a href="https://run.claw.cloud/">https://run.claw.cloud</a><br>github注册时间：<a href="https://api.github.com/users/%E7%94%A8%E6%88%B7%E5%90%8D">https://api.github.com/users/用户名</a></p></blockquote><p><img src="/img/clawcloud-1.png" alt="首页"></p><h2 id="alist"><a href="#alist" class="headerlink" title="alist"></a>alist</h2><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs routeros">一些参数：<br>xhofe/alist:latest<br><br><span class="hljs-attribute">PUID</span>=0 <br><span class="hljs-attribute">PGID</span>=0<br><span class="hljs-attribute">UMASK</span>=022<br><br>/opt/alist/data<br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>好软推荐</category>
      
    </categories>
    
    
    <tags>
      
      <tag>部署</tag>
      
      <tag>免费</tag>
      
      <tag>clawcloud</tag>
      
      <tag>容器</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>基于RSS的信息聚合软件</title>
    <link href="/2025/06/03/%E5%9F%BA%E4%BA%8ERSS%E7%9A%84%E4%BF%A1%E6%81%AF%E8%81%9A%E5%90%88%E8%BD%AF%E4%BB%B6/"/>
    <url>/2025/06/03/%E5%9F%BA%E4%BA%8ERSS%E7%9A%84%E4%BF%A1%E6%81%AF%E8%81%9A%E5%90%88%E8%BD%AF%E4%BB%B6/</url>
    
    <content type="html"><![CDATA[<blockquote><p>这是一个免费的信息聚合软件，有了它，不用看广告，不用来回切换各种APP就能快速获取资讯，订阅博主。在这个资讯爆炸的时代，他可以极大提高信息收集效率，它还可以摆脱算法推荐，打破信息茧房，重新做信息的主人而不是奴隶。</p></blockquote><ul><li>官网下载：<a href="https://follow.is/">https://follow.is/</a></li><li>github下载：<a href="https://github.com/RSSNext/Folo/releases/">https://github.com/RSSNext/Folo/releases/</a></li><li>视频教程：<a href="https://www.bilibili.com/video/BV1S2UDYWEKs">https://www.bilibili.com/video/BV1S2UDYWEKs</a><br>或者<br><a href="https://www.youtube.com/watch?v=oz3GBiLVzWo">https://www.youtube.com/watch?v=oz3GBiLVzWo</a></li><li>微信公众号转RSS：WeWe RSS</li><li>任何网站转Rss：RssHub</li></ul>]]></content>
    
    
    <categories>
      
      <category>好软推荐</category>
      
    </categories>
    
    
    <tags>
      
      <tag>RSS 聚合 软件</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Scrapy基础</title>
    <link href="/2025/06/02/Scrapy%E5%9F%BA%E7%A1%80/"/>
    <url>/2025/06/02/Scrapy%E5%9F%BA%E7%A1%80/</url>
    
    <content type="html"><![CDATA[<h2 id="测试一下子分类"><a href="#测试一下子分类" class="headerlink" title="测试一下子分类"></a>测试一下子分类</h2>]]></content>
    
    
    <categories>
      
      <category>爬虫</category>
      
      <category>scrapy</category>
      
    </categories>
    
    
    <tags>
      
      <tag>scrapy</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>5.文章查看pdf文件</title>
    <link href="/2025/06/01/5-%E6%96%87%E7%AB%A0%E6%9F%A5%E7%9C%8Bpdf%E6%96%87%E4%BB%B6/"/>
    <url>/2025/06/01/5-%E6%96%87%E7%AB%A0%E6%9F%A5%E7%9C%8Bpdf%E6%96%87%E4%BB%B6/</url>
    
    <content type="html"><![CDATA[<h2 id="安装插件"><a href="#安装插件" class="headerlink" title="安装插件"></a>安装插件</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ npm install --save hexo-pdf<br></code></pre></td></tr></table></figure><p>官网：<a href="https://github.com/superalsrk/hexo-pdf">https://github.com/superalsrk/hexo-pdf</a></p><h2 id="使用"><a href="#使用" class="headerlink" title="使用"></a>使用</h2><ol><li><p>在本地的根目录下打开source在这个文件夹下直接将你的pdf放进去即可</p></li><li><p>文章中使用如下代码</p><figure class="highlight django"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs django"><span class="hljs-template-tag">&#123;% <span class="hljs-name">pdf</span> /你自己刚刚放的pdf文档的名称.pdf %&#125;</span><br></code></pre></td></tr></table></figure></li></ol>]]></content>
    
    
    <categories>
      
      <category>Hexo</category>
      
    </categories>
    
    
    <tags>
      
      <tag>pdf</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>4.hexo博文加密</title>
    <link href="/2025/06/01/4-hexo%E5%8D%9A%E6%96%87%E5%8A%A0%E5%AF%86/"/>
    <url>/2025/06/01/4-hexo%E5%8D%9A%E6%96%87%E5%8A%A0%E5%AF%86/</url>
    
    <content type="html"><![CDATA[<div class="hbe hbe-container" id="hexo-blog-encrypt" data-wpm="Oh, this is an invalid password. Check and try again, please." data-whm="OOPS, these decrypted content may changed, but you can still have a look.">  <script id="hbeData" type="hbeData" data-hmacdigest="c74a3cf88c17dbca427356408079e2f8114ff70e95d2436f42df5a5076d58ab4">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</script>  <div class="hbe hbe-content">    <div class="hbe hbe-input hbe-input-default">      <input class="hbe hbe-input-field hbe-input-field-default" type="password" id="hbePass">      <label class="hbe hbe-input-label hbe-input-label-default" for="hbePass">        <span class="hbe hbe-input-label-content hbe-input-label-content-default">Hey, password is required here.</span>      </label>    </div>  </div></div><script data-pjax src="/lib/hbe.js"></script><link href="/css/hbe.style.css" rel="stylesheet" type="text/css">]]></content>
    
    
    <categories>
      
      <category>Hexo</category>
      
    </categories>
    
    
    <tags>
      
      <tag>加密</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>3.优化压缩图片</title>
    <link href="/2025/05/25/3.%E4%BC%98%E5%8C%96%E5%8E%8B%E7%BC%A9%E5%9B%BE%E7%89%87/"/>
    <url>/2025/05/25/3.%E4%BC%98%E5%8C%96%E5%8E%8B%E7%BC%A9%E5%9B%BE%E7%89%87/</url>
    
    <content type="html"><![CDATA[<h1 id="hexo-all-minifier-插件"><a href="#hexo-all-minifier-插件" class="headerlink" title="hexo-all-minifier 插件"></a>hexo-all-minifier 插件</h1><h2 id="1-安装插件"><a href="#1-安装插件" class="headerlink" title="1. 安装插件"></a>1. 安装插件</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">npm install hexo-all-minifier --save<br></code></pre></td></tr></table></figure><h2 id="2-配置插件"><a href="#2-配置插件" class="headerlink" title="2. 配置插件"></a>2. 配置插件</h2><p>在 Hexo 根目录下的 _config.yml 文件中添加以下配置：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-attr">hexo-all-minifier:</span><br>  <span class="hljs-attr">css:</span> <br>    <span class="hljs-attr">enabled:</span> <span class="hljs-literal">true</span><br>    <span class="hljs-attr">advanced:</span> <span class="hljs-literal">false</span>   <span class="hljs-comment"># 设为true会进行更彻底的压缩但可能耗时更长</span><br>    <span class="hljs-attr">exclude:</span>          <span class="hljs-comment"># 排除不需要压缩的CSS文件</span><br>      <span class="hljs-bullet">-</span> <span class="hljs-string">&#x27;*.min.css&#x27;</span><br>      <br>  <span class="hljs-attr">js:</span><br>    <span class="hljs-attr">enabled:</span> <span class="hljs-literal">true</span><br>    <span class="hljs-attr">mangle:</span> <span class="hljs-literal">true</span>      <span class="hljs-comment"># 混淆变量名</span><br>    <span class="hljs-attr">output:</span> <br>    <span class="hljs-attr">compress:</span><br>    <span class="hljs-attr">exclude:</span>          <span class="hljs-comment"># 排除不需要压缩的JS文件</span><br>      <span class="hljs-bullet">-</span> <span class="hljs-string">&#x27;*.min.js&#x27;</span><br>      <br>  <span class="hljs-attr">html:</span><br>    <span class="hljs-attr">enabled:</span> <span class="hljs-literal">true</span><br>    <span class="hljs-attr">exclude:</span><br>      <br>  <span class="hljs-attr">image:</span><br>    <span class="hljs-attr">enabled:</span> <span class="hljs-literal">true</span>     <span class="hljs-comment"># 压缩图片</span><br>    <span class="hljs-attr">interlaced:</span> <span class="hljs-literal">false</span><br>    <span class="hljs-attr">multipass:</span> <span class="hljs-literal">false</span><br>    <span class="hljs-attr">optimizationLevel:</span> <span class="hljs-number">2</span><br>    <span class="hljs-attr">pngquant:</span> <span class="hljs-literal">false</span><br>    <span class="hljs-attr">progressive:</span> <span class="hljs-literal">false</span><br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>Hexo</category>
      
    </categories>
    
    
    <tags>
      
      <tag>优化</tag>
      
      <tag>压缩图片</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>2.部署到github</title>
    <link href="/2025/05/24/2-%E9%83%A8%E7%BD%B2%E5%88%B0github/"/>
    <url>/2025/05/24/2-%E9%83%A8%E7%BD%B2%E5%88%B0github/</url>
    
    <content type="html"><![CDATA[<h2 id="1-GitHub创建仓库"><a href="#1-GitHub创建仓库" class="headerlink" title="1.GitHub创建仓库"></a>1.GitHub创建仓库</h2><ol><li>进入GitHub主页，点击右上角的<code>+</code>号，选择<code>New repository</code>。</li><li>名字以github.io结尾，例如<code>username.github.io</code>。仓库名一定要与用户名一致。</li><li>回到 git bash黑窗口中，输入以下两个命令（逐条）：<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">git config --global user.name <span class="hljs-string">&quot;yourname&quot;</span><br>git config --global user.email <span class="hljs-string">&quot;youremail&quot;</span><br></code></pre></td></tr></table></figure></li><li>创建 ssh，输入命令，然后一直回车<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">ssh-keygen -t rsa -C <span class="hljs-string">&quot;youremail&quot;</span><br></code></pre></td></tr></table></figure></li><li>复制ssh公钥，打开<code>C:\Users\yourname\.ssh</code>目录，找到<code>id_rsa.pub</code>文件，用记事本打开，复制里面的内容。</li><li>在 GitHub的 Setting里面，找到 SSH keys，把 id_rsa.pub 里面的内容全部复制到 key 进去，title随便写一个就行</li></ol><h2 id="2-hexo部署到GitHub"><a href="#2-hexo部署到GitHub" class="headerlink" title="2.hexo部署到GitHub"></a>2.hexo部署到GitHub</h2><ol><li>在 blog文件夹下面找到 _config.yml 文件，末尾添加以下内容：<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># Deployment</span><br><span class="hljs-comment">## Docs: https://hexo.io/docs/one-command-deployment</span><br><span class="hljs-comment"># deploy:</span><br><span class="hljs-comment">#   type: &#x27;&#x27;</span><br>deploy:<br>  <span class="hljs-built_in">type</span>: git<br>  repo: git@github.com:usename/usename.github.io.git<br>  branch: main<br>  message: <span class="hljs-string">&quot;Site updated: &#123;&#123; now(&#x27;YYYY-MM-DD HH:mm:ss&#x27;) &#125;&#125;&quot;</span>  <span class="hljs-comment"># 取消注释并启用</span><br></code></pre></td></tr></table></figure></li><li>找到url，修改为自己的仓库地址，例如：<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs bash">url = url: https://github.com/username/username.github.io<br>```bash<br><br><span class="hljs-comment">## 3.安装git部署工具</span><br>1. 打开 git bash黑窗口，输入以下命令：<br>```bash<br>npm install hexo-deployer-git --save<br></code></pre></td></tr></table></figure></li><li>输入以下三行命令，生成静态文件，上传到GitHub仓库<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash">hexo clean<br>hexo g<br>hexo d<br></code></pre></td></tr></table></figure></li></ol><ul><li>如果成功则可以通过username.github.io访问自己的博客<blockquote><p>hexo clean清除了你之前生成的东西，也可以不加。 hexo generate 顾名思义，生成静态文章，可以用 hexo g缩写 hexo deploy 部署文章，可以用hexo d缩写</p></blockquote></li></ul><ol start="3"><li>如果是在离线端即 localhost:4000端测试你的博客，则只需要 <code>hexo g + hexo s</code> 即可，无需 hexo d</li></ol><blockquote><p>注意：<br>一定要保证github的仓库地址与配置文件中的地址一致，否则会出现错误。比如fatal: unable to access ‘<a href="https://github.com/xxxx/xxxx.github.io/'%E6%88%96%E8%80%85FATAL">https://github.com/xxxx/xxxx.github.io/&#39;或者FATAL</a> Something’s wrong. Maybe you can find the solution here: <a href="https://hexo.io/docs/troubleshooting.html">https://hexo.io/docs/troubleshooting.html</a><br>Error: Spawn failed（不关梯子或者加速器的事，找半天文章都误导我，因为网上一些配置的文章老了，原因是一些格式要一致）</p></blockquote>]]></content>
    
    
    <categories>
      
      <category>Hexo</category>
      
    </categories>
    
    
    <tags>
      
      <tag>部署</tag>
      
      <tag>github</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>1.本地搭建</title>
    <link href="/2025/05/23/1-%E6%9C%AC%E5%9C%B0%E6%90%AD%E5%BB%BA/"/>
    <url>/2025/05/23/1-%E6%9C%AC%E5%9C%B0%E6%90%AD%E5%BB%BA/</url>
    
    <content type="html"><![CDATA[<h1 id="本地创建默认的hexo博客"><a href="#本地创建默认的hexo博客" class="headerlink" title="本地创建默认的hexo博客"></a>本地创建默认的hexo博客</h1><blockquote><p>hexo是一款非常快速，简介，高效的博客框架平台，我们可以利用hexo快速生成博客网站的模板，然后部署为我们自己的博客网站。</p></blockquote><h2 id="1-前置准备"><a href="#1-前置准备" class="headerlink" title="1. 前置准备"></a>1. 前置准备</h2><ul><li><p>安装nodejs，链接：<a href="https://nodejs.org/zh-cn">https://nodejs.org/zh-cn</a></p></li><li><p>安装git，链接：<a href="https://git-scm.com/downloads">https://git-scm.com/downloads</a></p></li></ul><h2 id="2-快速搭建"><a href="#2-快速搭建" class="headerlink" title="2. 快速搭建"></a>2. 快速搭建</h2><ul><li>安装hexo<figure class="highlight mipsasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs mipsasm">npm <span class="hljs-keyword">install </span>hexo-cli -g<br>hexo init <span class="hljs-keyword">blog</span><br><span class="hljs-keyword"></span>cd <span class="hljs-keyword">blog</span><br><span class="hljs-keyword"></span>npm <span class="hljs-keyword">install</span><br><span class="hljs-keyword"></span>hexo server<br></code></pre></td></tr></table></figure></li></ul><p>打开浏览器，在浏览器输入 localhost:4000 即可进入你的初始默认博客网站。</p>]]></content>
    
    
    <categories>
      
      <category>Hexo</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Hexo</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Hello World</title>
    <link href="/2025/05/22/hello-world/"/>
    <url>/2025/05/22/hello-world/</url>
    
    <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo new <span class="hljs-string">&quot;My New Post&quot;</span><br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo server<br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo generate<br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo deploy<br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
    
    
    
  </entry>
  
  
  
  
</search>
